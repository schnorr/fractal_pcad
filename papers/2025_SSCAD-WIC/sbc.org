# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+TITLE: Dynamic Load Balancing and Scalability Analysis of the Mandelbrot Set in a Multi-Threaded HPC Application
#+AUTHOR: Francisco Pegoraro Etcheverria, Rayan Raddatz de Matos, Kenichi Brumati, Lucas Mello Schnorr

#+STARTUP: overview indent
#+LANGUAGE: en
#+OPTIONS: H:3 creator:nil timestamp:nil skip:nil toc:nil num:t ^:nil ~:~
#+OPTIONS: author:nil title:nil date:nil
#+TAGS: noexport(n) deprecated(d) ignore(i)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [12pt]

#+LATEX_HEADER: \sloppy

# PDF generation can be done by make (thanks Luka Stanisic)
#   or C-c C-e l p (thanks Vinicius Garcia)

* Chamada de Trabalhos SSCAD-WIC                                   :noexport:

O Workshop de Iniciação Científica em Arquitetura de Computadores e
Computação de Alto Desempenho (SSCAD-WIC) é um evento anual, realizado
em conjunto com o Simpósio em Sistemas Computacionais de Alto
Desempenho (SSCAD) desde 2007, oferecendo uma oportunidade para os
alunos de graduação apresentarem e discutirem seus trabalhos nos
tópicos de interesse do SSCAD.

Os artigos aceitos no evento serão publicados em formato digital e
apresentados apenas na modalidade oral. Os artigos poderão ser
redigidos em português ou inglês. O processo de submissão de trabalhos
é eletrônico através do sistema JEMS onde serão aceitos somente
arquivos no formato PDF. Os anais serão publicados na SBC OpenLib
(SOL).

Os três melhores artigos aceitos no SSCAD-WIC receberão premiação.
Datas Importantes

    Submissão de trabalhos:31/07/2025
    Notificação de aceitação: 19/09/2024
    Envio da versão final: 25/09/2024

Tópicos de Interesse

A chamada de trabalhos está aberta (mas não limitada) aos seguintes
tópicos de interesse:

    Algoritmos Paralelos e Distribuídos
    Aplicações de Computação de Alto Desempenho
    Big Data (fundamentos; infraestrutura; administração e gerenciamento; descoberta e mineração; segurança e privacidade; aplicações)
    Aprendizado de Máquina em Alto Desempenho
    Arquiteturas de Computadores
    Arquiteturas Avançadas, Dedicadas e específicas
    Avaliação, Medição e Predição de Desempenho
    Computação em Aglomerados de Computadores
    Computação Heterogênea
    Computação de Alto Desempenho em Grade e na Nuvem
    Computação Móvel de Alto Desempenho
    Computação Móvel, Pervasiva e Embarcada
    Computação Quântica
    Engenharia de Desempenho
    Escalonamento e Balanceamento de Carga
    Internet das Coisas (IoT)
    Linguagens, Compiladores e Ferramentas para Alto Desempenho
    Memória Compartilhada Distribuída (DSM)
    Modelagem e Simulação de Arquiteturas e Sistemas Paralelos/Sistemas Distribuídos
    Redes e Protocolos de Comunicação de Alto Desempenho
    Simulação de Arquiteturas e Sistemas Paralelos
    Sistemas de Arquivos e Entrada e Saída de Alto Desempenho
    Sistemas de Banco de Dados Paralelos e Distribuídos
    Sistemas de Memória
    Sistemas Operacionais
    Sistemas Tolerantes a Falhas
    Software Básico para Computação Paralela e Distribuída
    Técnicas e Métodos de Extração de Paralelismo
    Teste e Depuração de Programas Concorrentes
    Virtualização

Submissões

A submissão de artigos para o SSCAD-WIC 2025 deve ser feita pelo
sistema JEMS da SBC. Os artigos submetidos devem ser escritos em
português ou inglês e obedecer ao limite de 8 páginas (incluindo
figuras, tabelas e referências) seguindo o formato da SBC para
submissão de artigos.  Coordenação do SSCAD-WIC

    Gabriel P. Silva (Universidade Federal do Rio de Janeiro) — gabriel@ic.ufrj.br
    Samuel Ferraz (Universidade Federal de Mato Grosso do Sul) — samuel.ferraz@ufms.br

Comitê de Programa (a confirmar)

    Adenauer Yamin (Universidade Católica de Pelotas/Universidade Federal de Pelotas)
    Alexandre Carissimi (Universidade Federal do Rio Grande do Sul)
    Anderson Faustino (Universidade Estadual de Maringá)
    André Du Bois (Universidade Federal de Pelotas)
    Andriele Busatto do Carmo (Universidade do Vale do Rio dos Sinos)
    Arthur Lorenzon (Universidade Federal do Rio Grande do Sul)
    Calebe Bianchini (Universidade Presbiteriana Mackenzie)
    Claudio Schepke (Universidade Federal do Pampa)
    Dalvan Griebler (Pontifícia Universidade Católica do Rio Grande do Sul)
    Diego Leonel Cadette Dutra (Universidade Federal do Rio de Janeiro)
    Edson Tavares de Camargo (Universidade Tecnológica Federal do Paraná)
    Edson Luiz Padoin (Universidade Regional do Noroeste do Estado do Rio Grande do Sul)
    Edward Moreno (Universidade Federal de Sergipe)
    Emilio Francesquini (Universidade Federal do ABC)
    Fabíola M. C. de Oliveira (Universidade Federal do ABC)
    Fabrício Góes (University of Leicester)
    Gabriel Nazar (Universidade Federal do Rio Grande do Sul)
    Gabriel P. Silva (Universidade Federal do Rio de Janeiro)
    Gerson Geraldo H. Cavalheiro (Universidade Federal de Pelotas)
    Guilherme Galante (Universidade Estadual do Oeste do Paraná)
    Guilherme Koslovski (Universidade do Estado de Santa Catarina)
    Hélio Guardia (Universidade Federal de São Carlos)
    Henrique Cota de Freitas (Pontifícia Universidade Católica de Minas Gerais)
    Hermes Senger (Universidade Federal de São Carlos)
    João Fabrício Filho (Universidade Tecnológica Federal do Paraná)
    Jorge Barbosa (Universidade do Vale do Rio dos Sinos)
    José Saito (Universidade Federal de São Carlos/Centro Universitário Campo Limpo Paulista)
    Josemar Souza (Universidade do Estado da Bahia)
    Joubert Lima (Universidade Federal de Ouro Preto)
    Juliano Foleiss (Universidade Tecnológica Federal do Paraná)
    Kalinka Castelo Branco (Instituto De Ciências Matemáticas e de Computação – USP)
    Leonardo Pinho (Universidade Federal do Pampa)
    Liana Duenha (Universidade Federal de Mato Grosso do Sul)
    Lucas Mello Schnorr (Universidade Federal do Rio Grande do Sul)
    Lucas Wanner (Universidade Estadual de Campinas)
    Luciano Senger (Universidade Estadual de Ponta Grossa)
    Luis Carlos De Bona (Universidade Federal do Paraná)
    Luiz Carlos Albini (Universidade Federal do Paraná)
    Marcelo Lobosco (Universidade Federal de Juiz de Fora)
    Marcio Oyamada (Universidade Estadual do Oeste do Paraná)
    Marco Wehrmeister (Universidade Tecnológica Federal do Paraná)
    Marco Antonio Zanata Alves (Universidade Federal do Paraná)
    Marcus Botacin (Texas A&M University)
    Maria Clicia Castro (Universidade Estadual do Rio de Janeiro)
    Mario Dantas (Universidade Federal de Juiz de Fora)
    Mateus Rutzig (Universidade Federal de Santa Maria)
    Matheus Souza (Pontifícia Universidade Católica de Minas Gerais)
    Márcio Castro (Universidade Federal de Santa Catarina)
    Márcio Kreutz (Universidade Federal do Rio Grande do Norte)
    Monica Pereira (Universidade Federal do Rio Grande do Norte)
    Nahri Moreano (Universidade Federal de Mato Grosso do Sul)
    Newton Will (Universidade Tecnológica Federal do Paraná)
    Odorico Mendizabal (Universidade Federal de Santa Catarina)
    Omar Cortes (Instituto Federal do Maranhão)
    Paulo Cesar Santos (Universidade Federal do Paraná)
    Rafaela Brum (Universidade Federal Fluminense)
    Renato Ishii (Universidade Federal de Mato Grosso do Sul)
    Ricardo da Rocha (Universidade Federal de Catalão)
    Ricardo Menotti (Universidade Federal de São Carlos)
    Rodolfo Azevedo (Universidade Estadual de Campinas)
    Rodrigo Campiolo (Universidade Tecnológica Federal do Paraná)
    Rodrigo Righi (Universidade do Vale do Rio dos Sinos)
    Rogério Gonçalves (Universidade Tecnológica Federal do Paraná)
    Samuel Ferraz (Universidade Federal do Mato Grosso do Sul)
    Sairo Santos (Universidade Federal Rural do Semi-Árido)
    Sarita Bruschi (Instituto de Ciências Matemáticas e de Computação – USP)
    Sergio Carvalho (Universidade Federal de Goiás)
    Tiago Ferreto (Pontifícia Universidade Católica Rio Grande do Sul)
    Tiago Heinrich (Universidade Federal do Paraná)
    Vinícius Vitor dos Santos Dias (Universidade Federal de Lavras)
    Vinícius Garcia (Universidade Federal do Paraná)
    Vinícius Garcia Pinto (Universidade Federal do Rio Grande)
    Wagner Zola (Universidade Federal do Paraná)
    Wanderson Roger Azevedo Dias (Instituto Federal de Rondônia)

Patrocinadores:
Diamante:

Parceiro:
Organização:
Promoção:
Financiamento:

    Chamada de Trabalhos – Trilha Principal Chamada de Trabalhos –
    Workshop sobre Educação em Arquitetura de Computadores (WEAC)
    Chamada de Trabalhos SSCAD-WIC Comitês Concurso de Teses e
    Dissertações em Arquitetura de Computadores e Computação de Alto
    Desempenho (SSCAD-CTD) Hospedagem Local Minicursos Principal

Copyright ©2025 XXVI SSCAD 2025 . All rights reserved. Powered by
WordPress & Designed by Bizberg Themes

* *The Paper*                                                       :ignore:
** Frontpage                                                        :ignore:

#+BEGIN_EXPORT latex
\makeatletter
\let\orgtitle\@title
\makeatother
\title{\orgtitle}

\author{
Francisco Pegoraro Etcheverria\inst{1},
Rayan Raddatz de Matos\inst{1},\\
Kenichi Brumati\inst{1},
Lucas Mello Schnorr\inst{1}
}

\address{Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS)\\
   Caixa Postal 15.064 -- 91.501-970 -- Porto Alegre -- RS -- Brazil}
#+END_EXPORT

#+LaTeX: \maketitle

** Abstract                                                         :ignore:

#+begin_abstract
This work investigates load balancing and scalability in
High-Performance Computing (HPC) systems using the Mandelbrot Set as a
benchmark. Due to its irregular and computationally intensive workload, the
Mandelbrot Set is ideal for evaluating dynamic workload distribution
strategies. We present a custom MPI-based implementation employing a
client-server model with dynamic load balancing, where idle workers
request new work from a coordinator. Our contributions include
implementing this parallel application and a performance analysis
highlighting load imbalance and system scalability.  The main results
indicate that the granularity factor significantly influences load
balance, and its choice depends on the selected fractal region.
#+end_abstract

** Introduction

# *[Context/Load Balancing]*
High-Performance Computing (HPC) systems are increasingly complex and
extensive cite:dongarra2024. They accommodate applications that
frequently have irregular workloads and are computationally
demanding. One of the significant concerns when dealing with irregular
workloads is achieving a balanced workload distribution among the
computational resources available to maximize performance and resource
usage. The load balancing task has a significant role in the
application performance when executing it in the cluster, ensuring
that each process in the system does approximately the same work
during the program execution.

# *[Mandelbrot]*
A common activity to study the correct usage of an HPC system is to
verify if the application achieves adequate load balancing amongst its
processes. This is particularly challenging for
irregular workloads, as their dynamic nature makes standard
static partitioning techniques unsuitable. The Mandelbrot Set
cite:mandelbrot1980 is a typical application with an irregular
workload because different points require varying numbers 
of iterations to compute. For each 
point $c$ in the complex plane, the application iteratively applies the function 
$f(z) = z^2 + c$, starting from $z =0$. $c$ belongs to the Mandelbrot 
Set if the computation remains bounded, that is, if its magnitude does 
not exceed a fixed radius within a limited number of iterations. To 
generate a 2D image of the set, the application performs this 
computation for every pixel corresponding to a point in the complex plane. 

# *[What is this work?/What we will do about the things we introduced?]*
The Mandelbrot Set is a well-known compute-bound application and is
considered embarrassingly parallel, as all point calculations are
independent. Therefore, being highly parallelizable and
computationally intensive, it often serves as a benchmark for HPC to
measure the performance of a computational system. In this paper, we
implement an MPI version of this application from scratch with a
dynamic load balancer where workers, when idle, contact the
coordinator for a new set of points to work on. We then study the load
balancing and scalability of this HPC application. Our significant
contributions include: (1) a multi-threaded application that
implements the Mandelbrot Set using a client-server architecture where
the server is an MPI application with dynamic load-balancing, (2) a
performance analysis with metrics to provide an overview and load
imbalance perspectives. Our main results indicate that the granularity
factor significantly influences load balance, and its choice depends
on the selected fractal region and the severity of how unbalanced the
load is.

This paper has the following organization. Section [[sec.related]]
presents other load balancing performance analysis of the Mandelbrot
set. Section [[sec.materials]] discusses materials and methods of our
work, including a detailed description of the observed
system. Section [[sec.results]] presents the experimental
results. Finally, Section [[sec.conclusion]] concludes this text with
future work.

#+latex: \noindent
*Software and Data Availability*. We endeavor to make our analysis
reproducible for a better science. We made available a companion
material hosted in a public GitHub repository at
#+latex: {\scriptsize\url{https://github.com/schnorr/fractal_pcad/tree/main/papers/2025_SSCAD-WIC/companion}}.
Our companion contains the source code of this article and the
software necessary to handle the created datasets. We also include
instructions to run the experiment and figures.
# An archive is also available in Zenodo at URL.

** Related Work
<<sec.related>>

# *[References about load balancing]*
Load balancing is a widely studied field. Chenzhong and Francis
cite:xu2007load provide a general overview of load balancing
techniques for parallel computers. Sandeep cite:sandeep2008 more
specifically analyzes different load balancing algorithms and
concludes that while dynamic distributed load balancing algorithms are
generally considered better, the static algorithms are more
stable and predictable.  Another work cite:mohammed2020two proposes a
two-level dynamic load balancing to scientific applications that
operate with MPI+OpenMP and uses the Mandelbrot application as one of
its test cases because of the application's irregularity.
#+latex: %
#+latex: % *[References about the mandelbrot implementation]*
The Mandelbrot Set as an HPC application has also been vastly studied
and used as a benchmark in various works for its computationally
intensive, parallelizable, and irregular nature. For example, 
considering the Mandelbrot Set, Gomez cite:gomez2020mpi has
conducted a case study comparing MPI against OpenMP. Another work
cite:ozen2015exploring has specifically explored dynamic parallelism
in OpenMP.  Yang cite:yang2011performance also studies specific
parameters to control load balancing among application processes.
#+latex: %
The related work demonstrates how pertinent the Mandelbrot Set is as a
benchmarking application. Again, our work employs this application
with a strong focus on performance analysis, general metrics (mean
client time, speedup, and efficiency), and a metric specific to
quantifying load imbalance (imbalance percentage), as we detail next.

** Materials and Methods
<<sec.materials>>
*** The multi-threaded client--server MPI application for the Mandelbrot Set

The implemented multi-threaded system adopts a Client--Server
architecture designed to parallelize the computation of the Mandelbrot
Set while enabling efficient load balancing across multiple computing
nodes. Figure [[fig:system-architecture]] illustrates the overall
architecture, highlighting the main threads, communication queues, and
data flow between components. The Client is responsible for managing
user interactions and rendering the fractal image produced by the
Server. When the user requests a new region, the client issues a
/payload/ to the Server. The Server comprises a central MPI coordinator
which receives /payloads/ from the client, discretizes the
workload into smaller problems, and dynamically distributes these
smaller problems to a pool of MPI workers by demand. When the
Server starts, workers approach the coordinator to request work. Upon 
receiving a smaller problem, the workers carry out the numerical
computations for them (Mandelbrot Set) before sending
the /responses/ to the coordinator, which forwards them to the 
Client.

#+CAPTION: Multi-threaded system overview with processes, threads, and queues.
#+NAME: fig:system-architecture
[[./figures/system_architecture.png]]


Each interaction between the Client and the Server consists of the
exchange of /payload/ and /response/ objects. A /payload/ is a data
structure that specifies the region of the Mandelbrot Set to be
computed, including the bounds in the complex plane (given in 
~long double~ precision), the corresponding
screen coordinates, the /depth/, which is the maximum number of iterations
to apply in the Mandelbrot algorithm, and the /granularity/, which
determines the size of the square blocks into which the workload is partitioned. 
For example, a granularity of 5 means that
the Server will split the fractal space into several 5\times5 square
blocks. Each payload also includes an increasing generation number to
identify it in the case the Client sends several /payloads/ one after
another.
#+latex: %
The Server replies to a single /payload/ with several /response/ objects,
each carrying its corresponding payload, as well as the calculated
depth count at each pixel position. In addition to the depth counts,
the responses also include some metadata, such as the id of the worker
that computed it. By delivering results block by block, the Server
enables the Client to view the partial fractal regions without waiting
for the entire computation to complete.

We designed the Client to be responsive and highly interactive. As
shown in Figure [[fig:system-architecture]], the Client contains four
concurrent threads. The ~Main~ thread manages both rendering of the
fractal image and collection of user mouse and keyboard input. When a
new region is selected, ~Main~ constructs the corresponding payload and
pushes it to a dedicated queue.  The ~SendPayload~ thread dequeues payloads
from this queue and transmits them to the Server over a TCP
connection. Meanwhile, ~RecvResponse~ listens for incoming responses, and
enqueues them into a response queue. Finally, the ~ProcessResponse~
thread retrieves these responses and integrates them into the
displayed image by applying a coloring function to the calculated
depth count for each pixel, updating the pixel buffer
incrementally as results arrive.

On the Server side, the ~RecvPayload~ thread listens for Client payloads,
forwarding them to the ~DiscretizePayloads~ thread, which divides the
requested region into several payloads sized according to the
specified granularity. These are then placed into a queue, with
outdated payloads being discarded to prevent workers from computing
regions that are no longer relevant. As workers become available, they
request a new payload from the coordinator. The ~SendToWorker~ thread
dynamically assigns them payloads from the queue. Each worker
independently computes a response, producing the depth counts for all
pixels in that subregion. Once the response becomes ready, it is sent to
the ~RecvFromWorker~ thread, which enqueues it to a response queue. These
responses are then collected by ~SendResponse~, which sends them back to the
Client.

*** Hardware & Software configuration

We run all experiments at the /Parque Computacional de Alto Desempenho/
(PCAD) at INF/UFRGS. The Client executes on a single /draco/ node, while
the server executes on one to six /cei/ nodes. The draco node on which the
Client executes has two Intel Xeon E5‑2640 v2 processors at 2.00 GHz.
Each cei node, used for the compute-bound part, has two Intel Xeon
Silver 4116 processors at 2.10 GHz, providing 24 physical cores each
for a total of 144 physical cores. In all experiments, we have
exclusive access to the machines without any type of
virtualization. We also use the /performance/ frequency governor of the
=acpi-cpufreq=.  The MPI implementation was OpenMPI version 4.1.4 and
the Linux Kernel 6.1.0 with SMP support as released by the Debian 12
distribution. The Client--Server Ethernet network is 1Gbps, while the
MPI application executes in a 10Gbps Ethernet switch.

*** Experimental Project

We designed a set of experiments with various input parameters to
evaluate the performance, scalability and load balancing of the
application on the target system. These parameters were chosen to test
different computational characteristics of the application, enabling
us to assess how the system behaves under different workloads. The
experiments consisted of rendering fractal images with a resolution of
1920\times1080 pixels.  Each
execution is the combination of a value of the following factors:
Granularity, Number of Nodes, and Fractal Cases. The *Granularity*
factor has the six levels: [5\times5, 10\times10, 20\times20, 40\times40, 60\times60, 120\times120],
respectively resulting in [82944, 20736, 5184, 1296, 576, 144] tasks
for workers. Smaller blocks improve load balancing but increase
communication overhead. Larger blocks may lead to severe load
imbalance. The *Number of Nodes* factor varies from 1 to 6, with each
/node/ contributing 24 physical cores to the server.  This
corresponds to a total of 24 to 144 MPI ranks (steps of 24),
enabling the evaluation of scalability. Finally, *Fractal Cases* has three levels:
[easy, default, hard]. Figure [[fig:fractal-regions]] illustrates
representative images of each region. The /easy/ (maximum depth
of 1024) depicts a region where most points escape in only a few
iterations, testing the communication overhead, rather than
computational speed. The /default/ (150000) depicts a typical unbalanced
Mandelbrot fractal region, containing both points that are
computationally intensive, as well as many points that escape quickly,
stressing load balancing. Finally, the /hard/
(300000) depicts a deep region that is computationally intensive but
balanced, to assess throughput. Max depth values were selected to keep 
execution time bounded.

#+CAPTION: The three fractal cases, with the corresponding maximum depth values.
#+NAME: fig:fractal-regions
#+ATTR_LATEX: :placement [htbp]
\begin{figure}[htbp]
\centering
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{./figures/region_easy.png}
\caption*{easy (1024)}
\end{minipage}%\hfill
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{./figures/region_default.png}
\caption*{default (150000)}
\end{minipage}%\hfill
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{./figures/region_hard.png}
\caption*{hard (300000)}
\end{minipage}
\end{figure}

With these factors, we adopt a Full Factorial Design cite:jain1990art, enabling the
exploration of all possible combinations of factors, resulting in 108
distinct configurations (6\times6\times3). Each configuration has been executed
ten times so we can assess the experimental variability, and the
execution order has been randomized to avoid potential bias.
#+LATEX: %
All experiments consider a simplified Client as we
executed everything in the cluster without a graphical interface. Our
textual Client instead receives parameters through the command
line. The ~ProcessResponse~ thread is therefore absent, and the ~Main~
thread enqueues the payload and dequeues responses from the ~RecvResponse~
thread.

**** Code                                                       :noexport:
#+begin_src R :results output :session *R* :exports none :noweb yes :colnames yes
options(crayon.enabled=FALSE)
library(DoE.base)
library(tidyverse)

fator_granularity = c(5, 10, 20, 40, 60, 120)
fator_nodes = 1:6
fator_coordinates = c("easy", "default", "hard")

fac.design(nfactors = 3,
           replications = 10,
           repeat.only = FALSE,
           randomize = TRUE,
           seed=0,
           nlevels=c(length(fator_granularity),
                     length(fator_nodes),
                     length(fator_coordinates)),
           factor.names=list(
             granularity = fator_granularity,
             nodes = fator_nodes,
             coordinates = fator_coordinates
           )) |>
  as_tibble() |>
  mutate(resolution = '1920x1080') |>
  mutate(depth = case_when(coordinates == "easy" ~ "X",
                           coordinates == "default" ~ "Y",
                           coordinates == "hard" ~ "Z")) |>
  mutate_at(vars(granularity:depth), as.character) |>
  select(granularity, nodes, coordinates, depth, resolution, Blocks) |>
  write_csv("projeto_experimental_francisco.csv", progress=FALSE)
#+end_src

#+RESULTS:
: creating full factorial with 108 runs ...

*** Observability

We manually instrument the code of the Client and Server to collect
and combine specific events and derive both execution time and load
balancing metrics. In the Client, we register the elapsed time between
the creation of each payload and the arrival of the first response, as
well as the last response. These metrics enable us to verify the
latency of the application as well as total perceived time from the
user perspective. In the server, we measured the time between a
payload being received and its discretization, the first and last
responses being received by the ~RecvFromWorker~ thread, and the moments these
responses are sent to the Client in the ~SendResponse~ thread. This
information allow us to verify the discretization cost, and the amount
of compute time from the perspective of the coordinator. Finally, in
each MPI worker, we measured the individual times to compute each
payload, their pixel and depth counts, as well as the aggregate of
these values, allowing us to assess load balancing.

** Results
<<sec.results>>

We present the performance evaluation of our multi-threaded MPI
application based on the experiments described earlier. We focus on
four key metrics: the mean client time, speedup, efficiency, and
imbalance percentage. The *Mean Client Time* represents the total time
taken for the Client to receive the fully computed fractal for each
case (/payload/). The *Speedup* measures the ratio of the mean Client time with 
a single node for a given case and granularity setting to the mean Client 
time with another number of nodes for that same setting. That is, for a given number of 
nodes $n$, $S(n) = \frac{T(1)}{T(n)}$. We emphasize that our speedup
metric is relative to the number of nodes rather than processors. 
#+latex: %
Our server architecture is asymmetric, which necessitates a careful definition of 
ideal performance and efficiency. The baseline configuration on a single node uses 
23 workers and one coordinator, while each additional node contributes 24 workers. 
This results in a worker count for $n$ nodes of $24n - 1$.
Standard efficiency calculations using node count would yield misleading values above 
1.0 due to this uneven worker distribution.
Therefore, we normalize our metrics based on worker count rather than node count. 
We define the ideal speedup as $S_{ideal}(n) = \frac{24n - 1}{23}$, and *Efficiency*
as $E(n) = \frac{S(n)}{S_{ideal}(n)}$.
This method of computing $S$ and $E$ ensures that perfect linear scaling as workers are added results in efficiency = $1.0$, 
enabling fair comparison across configurations.
#+latex: %
Finally, the *Imbalance Percentage*
cite:derose2007detecting depicts how unevenly the computational
workload is distributed among workers. Lower values are better. It is
calculated as:
#+begin_export latex
\begin{equation}
\text{Imbalance Percentage} = \frac{L_{\text{max}} - L_{\text{avg}}}{L_{\text{max}}} \times \frac{n}{n-1}
\end{equation}
#+end_export
where $L_{\text{max}}$ is the computation time of the slowest worker,
$L_{\text{avg}}$ is the average computation time across all workers, and
$n$ is the number of workers. We report the mean value across the 10 trials.
#+latex: %
In our analysis we focus solely on Client times, which directly
reflect user-perceived performance, as the coordinator metrics closely
mirror client-side values. We also focus on worker-level timings,
which reveal the degree of load balancing achieved.

Figures [[fig:client-time]], [[fig:client-speedup]] and [[fig:client-efficiency]]
depict the time, speedup and efficiency results. We see that
performance appears to scale well with the addition of nodes for the
/default/ and /hard/ cases, provided an adequate granularity (nor low nor
high). The granularity 20 appears to be the best, with an efficiency
of around 0.98 with 6 nodes in the /hard/ case, and approximately 0.85 in the /default/
case. This is likely due to it presenting a good trade‑off
between the payload size and the number of payloads, with small
communication overhead while providing good load balancing.

#+CAPTION: Mean Client time for each case, with 99% CI error bars.
#+NAME: fig:client-time
[[./figures/client_time.png]]

#+CAPTION: Speedup and ideal speedup for each case.
#+NAME: fig:client-speedup
[[./figures/client_speedup.png]]


This interpretation can be confirmed in Figure
[[fig:imbalance-percentage]], which shows generally better load balancing
for lower granularities. The load balancing at higher granularity values 
tends to degrade as the number of nodes increases. The
/default/ case in particular seems to suffer from more worker imbalance
than the /hard/ case, due to the fractal region having a mix of very
easy and very hard regions.

In contrast, the /easy/ case shows a different trend: higher
granularities perform better, and increasing node counts
worsen performance. Because most points in this region escape in only
a few iterations, computation is inexpensive, and the bottleneck
is communication.  As such, lower granularities increase
overhead, which seems to worsen as more nodes are added. This
effect is especially visible at granularity 5 (see Figure
[[fig:client-time]] for instance): in the /default/ and /hard/ cases,
performance worsens past 3 nodes, nearly matching the times observed
in the /easy/ case. We conclude that the performance is limited by
communication rather than computation time at such low granularities.

#+CAPTION: Efficiency for each case.
#+NAME: fig:client-efficiency
[[./figures/client_efficiency.png]]




#+CAPTION: Mean Imbalance Percentage for each case, with 99% CI error bars.
#+NAME: fig:imbalance-percentage
[[./figures/imbalance_percentage.png]]




Imbalance is also high across granularities in the /easy/ case, as the
work is so light that some workers can finish a payload and request
another, while other workers are still waiting for their next payload.

** Conclusion
<<sec.conclusion>>

This work presented a dynamic, multi-threaded MPI-based implementation
of the Mandelbrot Set to study load balancing and scalability in HPC
systems. Through extensive experimentation, we demonstrated that
workload granularity plays a crucial role in performance, with optimal
values depending on the computational characteristics of the fractal
region.
#+latex: %
These results show that scaling depends on the balance between
computation and communication costs. For harder fractal regions, the
system scales very well with additional nodes when granularity is
appropriately chosen, with a granularity of 20 striking the best
balance. However, for simpler regions, communication overhead
dominates and additional nodes can even reduce performance.
#+latex: %
These insights highlight the importance of tuning granularity based on
workload characteristics to achieve efficient parallel execution.  As
future work, we plan to investigate varying granularity values based
on neighborhood fractal depth and its impact on performance and load
balance.

#+latex: \noindent
*Acknowledgments*.
We thank FAPERGS and CNPq for their financial support, which included
scientific initiation scholarships from both FAPERGS (PROBIC) and CNPq
(PBIC).  We also thank UFRGS for all institutional support. We also
extend our thanks to the Parallel and Distributed Processing Research
Group (GPPD) for access to the PCAD cluster resources.

** References                                                        :ignore:

# See next section to understand how refs.bib file is created.
bibliographystyle:sbc.bst
[[bibliography:refs.bib]]

* Bib file is here                                                 :noexport:

Tangle this file with C-c C-v t

* The SSCAD-WIC Reviews (Text)                                     :noexport:
** R1

The paper investigates load balancing and scalability in
High-Performance Computing systems, using a custom multi-threaded
MPI-based implementation of the Mandelbrot Set as a benchmark. The
authors analyze performance, demonstrating that workload granularity
critically influences load balance and system scalability, with
optimal granularity depending on the computational characteristics of
the fractal region being processed. The topic is within the scope of
SSCAD, is current and relevant, the contributions are clear, the text
is well-written and organized, cites future work, and the methodology
is straightforward. Some comments, questions, and suggestions follow:

1) Although the authors state the paper contributions clearly in the
   Introduction, they do not compare their paper with the literature
   and how they advance the state of the art.

2) The authors should update the references since there are too many
   too old ones, which may indicate outdated work.

Results:

3) The authors do not compare their approach with approaches in the
   literature that perform similar work, such as dynamically
   scheduling.

4) I could not check that, in the easy case, higher granularities
   perform better in Fig. 6. Perhaps it is a matter of the meaning of
   high x low granularities?

5) Besides that, the authors should include the 95% confidence
   interval in the results so we can compare, especially in Fig. 6.

6) The authors should analyse Figs. 4 and 5 more deeply, discussing
   linear speedup and weak or strong scalability.

7) How long did the experiments take? This value helps analyze if more
   than 10 executions would be feasible for an improved statistical
   analysis.

8) The authors should explain the sentence "We picked the median value
   from the 10 trials". Did the authors do that for all four metrics?

Writing: 

9) What does escape mean? The authors should explain this critical
   concept in the paper.

10) The authors should explain the meaning of draco and cei nodes.

11) The authors should review the grammar carefully, making verbal
    tenses consistent throughout the text.

12) The authors should define all acronyms, such as MPI.

** R2

O artigo estuda o impacto da granularidade de paralelismo na
escalabilidade e no balanceamento de carga de uma aplicação
embaraçosamente paralela e compute-bound.  O trabalho está muito bem
escrito, com objetivos claros, metodologia de experientação adequada e
reprodutível. A apresentação dos resultados também está clara e
objetiva.  De certa forma, as conclusões gerais do artigo confirmam o
que se espera de uma aplicação irregular, mas a novidade pode ser a
partir da arquitetura e implementação proposta. Esse último ponto não
ficou claro.  Apesar disso, é um trabalho de IC bem sólido e bem
apresentado.

No geral, deixo as seguintes sugestões:

1. Delimitar e contextualizar melhor as contribuições do trabalho
   frente aos relacionados. Outros trabalhos não consideram
   balanceamento de carga? Outros trabalhos não consideram uma
   arquitetura cliente-servidor? Isso não ficou muito claro.
2. No texto, é mencionada a metodologia de projeto
   fatorial. Entretanto, o impacto dos fatores (que, até onde entendo,
   seria uma saída importante dessa metodologia) não foi incluído nem
   mencionado.
3. Resolução da fig. 1 poderia ser melhorada
4. Figuras 3-6 poderiam se beneficiar de símbolos e padrões, além de
   cores, para facilitar a visualização.

** R3

O artigo investiga balanceamento de carga e escalabilidade em ambiente
distribuído, utilizando uma aplicação de Mandelbrot como
benchmark. São tratados os casos de escalonamento estático e dinâmico,
variando a granularidade de cálculo, considerando áreas do fractal com
diferentes custos computacionais. O trabalho em sí não apresenta
resultados inovadores, mas contribui como um "framework"
experimental. A disponibilidade de um repositório para reprodutividade
auxilia a tornar o material útil, dada a qualidade das informações que
constam no projeto. Caso aceito, sugiro aos autores considerar incluir
menção a possibilidade do uso da implementação como apoio
didático. Como sugestão de continuidade, sugiro considerar uma versão
da estratégia de escalonamento estática onde sejam tratadas diferentes
granularidades de blocos e envio intercalado de blocos aos
clientes. Outra possibilidade é desenvolver uma estratégia "roubo de
trabalho", na qual todo o trabalho inicia no nó 1 e os demais enviam
mensagens (a nós aleatório) para solicitar serviço. Ao receber uma
demanda, tendo serviço disponível, divide sua carga de trabalho como o
nó que solicitou. (workstealing, a lá Cilk).

** R4

O artigo apresenta uma avaliação de escalabilidade de um sistema
multi-threaded criado para a resolução do conjunto de
Mandelbrot. Destaco, como pontos fortes do trabalho: a) a excelente
clareza na apresentação e organização do trabalho (parabéns por
redigí-lo em inglês!); b) preocupação com reprodutibilidade dos
experimentos (os autores disponibilizaram um repositório com código
aberto, datasets e instruções disponíveis, algo louvável); c) métricas
bem justificadas (incluindo definição cuidadosa de eficiência, que
considera o desequilíbrio entre coordenador e workers). Contudo, vejo
alguns problemas.

Em relação à originalidade, justifico minha nota para os autores da
seguinte forma: vejo que o artigo repete um tema bastante explorado na
literatura (o próprio artigo cita vários trabalhos
semelhantes). Talvez possamos dizer que a contribuição original reside
principalmente na implementação específica em MPI com arquitetura
cliente-servidor e na análise detalhada de granularidade sob
diferentes regiões fractais. Embora haja alguma contribuição, trata-se
mais de uma variação aplicada de métodos já conhecidos do que de uma
proposta conceitualmente nova. Ademais, a análise se restringe ao
sistema desenhado pelos autores, de modo que tenho certa dificuldade
em extrapolar as conclusões para outras aplicações (com
características distintas de Mandelbrot) ou ambientes de software
(forma como o balanceador de carga foi projetado)/hardware(com
arquitura distinta da usada pelos autores).

A justificativa de minha nota para relevância se baseia na observação,
ponderada por tratar-se de uma conferência de IC, de que há alguma
contribuição: a avaliação de desempenho com projeto fatorial completo,
múltiplos níveis de granularidade e análise de escalabilidade fornece
informações úteis para práticas de programação paralela. Contudo, o
uso do Mandelbrot como benchmark limita um pouco a aplicabilidade
prática direta, já que é um problema não crítico, no meu entender, em
aplicações reais.

Não sei se preciso justificar para os autores a nota máxima no item
qualidade da apresentação, mas vou fazê-lo ainda assim. O artigo está
bem organizado, ainda mais considerando-se a limitação de espaço:
introdução clara, revisão relacionada adequada, seção de métodos
detalhada, apresentação de resultados com métricas bem
definidas. Gostei muito do artigo estar em inglês, com redação clara e
objetiva, e sem erros graves (listo alguns abaixo).

Vou passar algumas outras anotações que fiz. Vou apresentar como
tópicos para facilitar:

- * A originalidade pode ser melhorada usando outras aplicações
  irregulares mais realistas, como simulações de partículas, dinâmica
  molecular, algumas aplicações em grafos, etc. Acho Mandelbrot meio
  batido para estudos de balanceamento, apesar de entender que para IC
  pode ser uma porta de entrada interessante para introduzir o
  problema de balanceamento de carga. Até que ponto Mandelbrot pode
  ser generalizado para outras aplicações?

- * Falta uma discussão mais profunda sobre os porquês dos
  comportamentos observados, por exemplo comparando com trade-offs
  teóricos entre granularidade e sobrecarga de comunicação. Na própria
  parte dos métodos, não ficou claro quantas vezes vocês repetiram
  cada configuração (lembrem-se que o tempo é uma variável aleatória,
  e como tal, precisamos tratar adequadamente a sua variabilidade e
  reportar valores que nos levem a intervalos de confiança superiores
  à 95%).

- * Seria interessante incluir comparações diretas com implementações
  ou algoritmos de balanceamento dinâmico de outros trabalhos, além de
  apenas mencionar na revisão. Isso daria mais peso científico ao
  estudo. Sei que pela falta de espaço é o tipo de coisa que só pode
  ser feita em trabalhos futuros.

- * Bato muito com meus alunos que a hipótese de pesquisa tem de estar
  clara na seção de introdução. Acho que ficou meio implícita (a
  granularidade influencia fortemente o balanceamento e a
  escalabilidade). Poderia estar explicitada como hipótese formal.

- * A qualidade das figuras pode ser melhorada. Fiquei com a impressão
  que algumas figuras foram feitas no paint (figura 1). Melhorar a
  resolução.

- * Em relação ao inglês/typos: a) "Due to its irregular and
  compute-intensive workload" => "computationally intensive workload"
  não soa melhor? b) "An everyday activity to ..." => "A common
  activity..."? ou "A typical task..."? c) "f(z) = z2+c" => não seria
  z^2? d) "For example, all works..." => acho melhor "For example,
  considering the Mandelbrot Set, Gómez [2020] conducted..." e) "By
  reception of a smaller problem, the workers carry out the numerical
  computations..." => não é melhor algo como "Upon receiving a smaller
  problem, the workers carry out..."? f) "discretizes the
  workload..."=> olha, discretizar, para mim, é quando você transforma
  algo contínuo em discreto. Eu usaria outro termo, apesar de entender
  o que vocês quiseram falar: "divides", g) "before sending back to
  the Server the responses" => a ordem importa aqui: "before sending
  the responses back to the Server". h) "enabling the verification of
  all possible combinations of factor.." => verification? não seria
  "exploration"? i) "with granularity 20 striking the best balance."
  => "...with a granularity of 20 striking the best balance". Eu
  também não gosto muito do uso de article para falar de artigo, mas
  vejo que algumas pessoas usam, então troquem se acharem que devem:
  "In this article, we implement an MPI version of this application"
  => "In this paper, we implement an MPI version of this application"

Do ponto de vista técnico, algumas coisas talvez pudessem, em
trabalhos futuros, serem melhor discutidas: a) o servidor possui
várias threads que compartilham filas. O artigo não menciona
explicitamente o uso de sincronização e o seu custo; b) Não sei se
entendi corretamente, mas vocês descartam payloads considerados
antigos, não é? Se houver concorrência entre o descarte e a atribuição
de tarefas a um worker, pode ocorrer uso de payload inválido, não? Um
worker poderia calcular uma região que já não interessa, desperdiçando
recursos ou gerando resultados inconsistentes. Não fica claro se há
sincronização para garantir consistência entre descarte e distribuição
de novas tarefas; c) se entendi corretamente, granularidades pequenas
aumentam overhead de comunicação, o que faz sentido. Mas em cenários
extremos, esse overhead pode não apenas degradar o desempenho, mas
também causar gargalo no coordenador, já que ele é único e
centralizado. Isso pode limitar escalabilidade em sistemas maiores,
não? d) Não ficou muito clara a frequencia com que o cliente envia
payloads e recebe respostas. Dependendo das taxas, isso pode levar a
fila de mensagens acumuladas, causando latência extra. Aliás, pode
haver contended communication (todos os workers pedindo tarefas ao
mesmo coordenador ao mesmo tempo, por você ter um único
coordenador);e) o tempo de execução é muuuuuuito pequeno. Queria ver
tempos de execução maiores.

* Reviewer suggestions                                             :noexport:
** Figure 3
Text say "We picked the median value from the 10 trials."
Figure 3 caption say "Mean Client time"
** Introduction / Related Work
*** [R1,R2,R4] Explicitly compare contributions to related work and clarify how it advances state of the art
Impossible to do. It was never our goal to compare against other for
this particular work, as nobody does the architecture precisely the
way we do.
*** [R1,R2] Update references; add newer references
*** [R4] Explicitly state hypothesis about how granularity affects load balance/scalability
** Methodology / Implementation details
*** [R4] Clarify 10 repetitions of each setting
*** [R4] Mention queue thread safety/synchronization
*** [R4] Clarify handling of old/discarded payloads
*** TODO [R4] Discuss communication bottlenecks in the coordinator with small granularities
*** [R4] Clarify client payload send/receive frequency (only once per image)
*** [R4] Consider longer execution times for statistical significance
** Results / Analysis
*** [R1,R2,R4] Compare balancing approach and results with other works directly
Impossible to do.
*** [R1] Clarify worker imbalance at higher granularities in the easy case seen in Fig. 6 
(imbalance high but execution time low, likely due to only a few workers handling entire workload)
It is already there the discussion about these results.
*** DONE [R1,R4] Add 95% confidence intervals for results, especially Fig. 6. 
Added 99% CI

(Figure 3 has confidence intervals, but they are not visible)

Showing confidence intervals for the median is tricky. You could
instead replace all the median by the mean, killing median of all results reporting,
and then compute variability based on a 99% CI.

*** [R1] Analyse Figs. 4-5 more deeply, discuss linear speedup, weak/strong scalability
We only have strong scalability here. Discussion is enough (see Figures)
*** [R1] Report experiment execution times
We already do that
*** DONE [R1] Clarify median was used for imbalance, averages were used for other metrics
Perhaps replace all by the mean, killing the "median" reporting results
*** [R2] Include analysis of factor effects; clarify how each factor impacts metrics.
*** [R3] Consider mentioning potential educational use of the implementation.

** Figures / Visuals
*** DONE [R2,R4] Improve resolution of Fig. 1
*** [R2] Improve clarity of Figs. 3-6 (add symbols/patterns)
** Writing / Grammar / Terminology
*** [R1] Explain "escape" terminology
The "escape" is part of the fractal ways of working. 
*** [R1] Explain draco/cei nodes
They are already explained in Section 3.2 HW and SW Config.
*** DONE [R1] Review grammar consistency
*** [R1] Define all acronyms (MPI, etc.)
*** DONE [R4] "Due to its irregular and compute-intensive workload" => "computationally intensive workload"
*** DONE [R4] "An everyday activity to ..." => "A common activity..." / "A typical task..."
*** [R4] "f(z) = z2+c" =>  "f(z) = z^2+c"
*** DONE [R4] "For example, all works..." => "For example, considering the Mandelbrot Set, Gómez [2020] conducted..."
*** DONE [R4] "By reception of a smaller problem, the workers carry out the numerical computations..." => "Upon receiving a smaller problem, the workers carry out..."?
*** TODO [R4] "discretizes the workload..." => "divides"
*** DONE [R4] "before sending back to the Server the responses" => "before sending the responses back to the Server"
*** DONE [R4] "enabling the verification of all possible combinations of factor.." => "enabling the exploration of..."
*** DONE [R4] "with granularity 20 striking the best balance." => "...with a granularity of 20 striking the best balance"
*** DONE [R4] "In this article" =>  "In this paper"

** Future Work / Extensions
*** [R3] Evaluate static allocation approach with different granularities and interleaved distribution to workers
*** [R3] Evaluate work-stealing approach
*** [R4] Evaluate our approach with other (more realistic) irregular applications (particle simulation, molecular dynamics, graphs)
*** [R4] Have experiments with (much) longer execution times
* Bib file                                                         :noexport:
#+begin_src bibtex :tangle refs.bib
@book{jain1990art,
  title={The art of computer systems performance analysis},
  author={Jain, Raj},
  year={1990},
  publisher={john wiley \& sons}
}

  @article{yang2011performance,
    title={Performance-based parallel loop self-scheduling using hybrid OpenMP and MPI programming on multicore SMP clusters},
    author={Yang, Chao-Tung and Wu, Chao-Chin and Chang, Jen-Hsiang},
    journal={Concurrency and Computation: Practice and Experience},
    volume={23},
    number={8},
    pages={721--744},
    year={2011},
    publisher={Wiley Online Library}
  }

@inproceedings{ozen2015exploring,
author = {Ozen, Guray and Ayguade, Eduard and Labarta, Jesus},
title = {Exploring dynamic parallelism in OpenMP},
year = {2015},
isbn = {9781450340144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2832105.2832113},
doi = {10.1145/2832105.2832113},
abstract = {GPU devices are becoming a common element in current HPC platforms due to their high performance-per-Watt ratio. However, developing applications able to exploit their dazzling performance is not a trivial task, which becomes even harder when they have irregular data access patterns or control flows. Dynamic Parallelism (DP) has been introduced in the most recent GPU architecture as a mechanism to improve applicability of GPU computing in these situations, resource utilization and execution performance. DP allows to launch a kernel within a kernel without intervention of the CPU. Current experiences reveal that DP is offered to programmers at the expenses of an excessive overhead which, together with its architecture dependency, makes it difficult to see the benefits in real applications.In this paper, we propose how to extend the current OpenMP accelerator model to make the use of DP easy and effective. The proposal is based on nesting of teams constructs and conditional clauses, showing how it is possible for the compiler to generate code that is then efficiently executed under dynamic runtime scheduling. The proposal has been implemented on the MACC compiler supporting the OmpSs task--based programming model and evaluated using three kernels with data access and computation patterns commonly found in real applications: sparse matrix vector multiplication, breadth-first search and divide--and--conquer Mandelbrot. Performance results show speed-ups in the 40x range relative to versions not using DP.},
booktitle = {Proceedings of the Second Workshop on Accelerator Programming Using Directives},
articleno = {5},
numpages = {8},
keywords = {programming models, dynamic parallelism, compilers, OpenMP, OpenACC, OmpSs, GPGPU, CUDA},
location = {Austin, Texas},
series = {WACCPD '15}
}

  @article{dongarra2024,
    author    = {Dongarra, Jack and Keyes, David E.},
    title     = {The co-evolution of computational physics and high-performance computing},
    journal   = {Nature Reviews Physics},
    year      = {2024},
    url       = {https://www.nature.com/articles/s42254-024-00750-z}
  }


  @article{gomez2020mpi,
    title={MPI vs OpenMP: A case study on parallel generation of Mandelbrot set},
    author={G{\'o}mez, Ernesto Soto},
    journal={Innovation and Software},
    volume={1},
    number={2},
    pages={12--26},
    year={2020}
  }

@book{xu2007load,
author = {Xu, Chenzhong and Lau, Francis C. M.},
title = {Load Balancing in Parallel Computers: Theory and Practice},
year = {2013},
isbn = {1475770669},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {Load Balancing in Parallel Computers: Theory and Practice is about the essential software technique of load balancing in distributed memory message-passing parallel computers, also called multicomputers. Each processor has its own address space and has to communicate with other processors by message passing. In general, a direct, point-to-point interconnection network is used for the communications. Many commercial parallel computers are of this class, including the Intel Paragon, the Thinking Machine Cm-5, and the Ibm Sp2. Load Balancing in Parallel Computers: Theory and Practice presents a comprehensive treatment of the subject using rigorous mathematical analyses and practical implementations. The focus is on nearest-neighbor load balancing methods in which every processor at every step is restricted to balancing its workload with its direct neighbours only. Nearest-neighbor methods are iterative in nature because a global balanced state can be reached through processors' successive local operations. Since nearest-neighbor methods have a relatively relaxed requirement for the spread of local load information across the system, they are flexible in terms of allowing one to control the balancing quality, effective for preserving communication locality, and can be easily scaled in parallel computers with a direct communication network. Load Balancing in Parallel Computers: Theory and Practice serves as an excellent reference source and may be used as a text for advanced courses on the subject.}
}

  @inproceedings{mohammed2020two,
    title={Two-level dynamic load balancing for high performance scientific applications},
    author={Mohammed, Ali and Cavelan, Aur{\'e}lien and Ciorba, Florina M and Cabez{\'o}n, Rub{\'e}n M and Banicescu, Ioana},
    booktitle={SIAM Conference on Parallel Processing for Scientific Computing},
    year={2020},
  }

  @article{mandelbrot1980,
  author = {Mandelbrot, Benoit B.},
  title = { “Fractal Aspects of the Iteration of Z → z $\Lambda$(1-Z) for Complex $\Lambda$ and Z”},
  journal = {Annals of the New York Academy of Sciences},
  volume = {357},
  number = {1},
  pages = {249-259},
  year = {1980}
  }

  @article{sandeep2008,
    title     = {Performance Analysis of Load Balancing Algorithms},
    author    = {Sandeep Sharma and  Sarabjit Singh and  Meenakshi Sharma},
    country	= {},
    institution	= {},
    journal   = {International Journal of Civil and Environmental Engineering},
    volume    = {2},
    number    = {2},
    year      = {2008},
    pages     = {367},
    ee        = {https://publications.waset.org/pdf/5537},
    url   	= {https://publications.waset.org/vol/14},
    bibsource = {https://publications.waset.org/},
    issn  	= {eISSN: 1307-6892},
    publisher = {World Academy of Science, Engineering and Technology},
    index 	= {Open Science Index 14, 2008},
  }

@inproceedings{derose2007detecting,
  title={Detecting application load imbalance on high end massively parallel systems},
  author={DeRose, Luiz and Homer, Bill and Johnson, Dean},
  booktitle={European Conference on Parallel Processing},
  pages={150--159},
  year={2007},
  organization={Springer}
}

#+end_src
* Emacs setup                                                      :noexport:
# Local Variables:
# eval: (add-to-list 'load-path ".")
# eval: (require 'ox-extra)
# eval: (ox-extras-activate '(ignore-headlines))
# eval: (require 'org-ref)
# eval: (require 'doi-utils)
# eval: (add-to-list 'org-latex-packages-alist '("" "url") t)
# eval: (add-to-list 'org-latex-packages-alist '("" "sbc-template") t)
# eval: (add-to-list 'org-latex-packages-alist '("AUTO" "babel" t ("pdflatex")))
# eval: (setq org-latex-pdf-process (list "latexmk -pdf %f"))
# eval: (add-to-list 'org-export-before-processing-hook (lambda (be) (org-babel-tangle)))
# End:
