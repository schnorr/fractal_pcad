# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+TITLE: Dynamic Load Balancing and Scalability Analysis of the Mandelbrot Set in a Multi-Threaded HPC Application
#+AUTHOR: Francisco Pegoraro Etcheverria, Rayan Raddatz de Matos, Kenichi Brumati, Lucas Mello Schnorr

#+STARTUP: overview indent
#+LANGUAGE: en
#+OPTIONS: H:3 creator:nil timestamp:nil skip:nil toc:nil num:t ^:nil ~:~
#+OPTIONS: author:nil title:nil date:nil
#+TAGS: noexport(n) deprecated(d) ignore(i)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [12pt]

#+LATEX_HEADER: \sloppy

# PDF generation can be done by make (thanks Luka Stanisic)
#   or C-c C-e l p (thanks Vinicius Garcia)

* Chamada de Trabalhos SSCAD-WIC                                   :noexport:

O Workshop de Iniciação Científica em Arquitetura de Computadores e
Computação de Alto Desempenho (SSCAD-WIC) é um evento anual, realizado
em conjunto com o Simpósio em Sistemas Computacionais de Alto
Desempenho (SSCAD) desde 2007, oferecendo uma oportunidade para os
alunos de graduação apresentarem e discutirem seus trabalhos nos
tópicos de interesse do SSCAD.

Os artigos aceitos no evento serão publicados em formato digital e
apresentados apenas na modalidade oral. Os artigos poderão ser
redigidos em português ou inglês. O processo de submissão de trabalhos
é eletrônico através do sistema JEMS onde serão aceitos somente
arquivos no formato PDF. Os anais serão publicados na SBC OpenLib
(SOL).

Os três melhores artigos aceitos no SSCAD-WIC receberão premiação.
Datas Importantes

    Submissão de trabalhos:31/07/2025
    Notificação de aceitação: 19/09/2024
    Envio da versão final: 25/09/2024

Tópicos de Interesse

A chamada de trabalhos está aberta (mas não limitada) aos seguintes
tópicos de interesse:

    Algoritmos Paralelos e Distribuídos
    Aplicações de Computação de Alto Desempenho
    Big Data (fundamentos; infraestrutura; administração e gerenciamento; descoberta e mineração; segurança e privacidade; aplicações)
    Aprendizado de Máquina em Alto Desempenho
    Arquiteturas de Computadores
    Arquiteturas Avançadas, Dedicadas e específicas
    Avaliação, Medição e Predição de Desempenho
    Computação em Aglomerados de Computadores
    Computação Heterogênea
    Computação de Alto Desempenho em Grade e na Nuvem
    Computação Móvel de Alto Desempenho
    Computação Móvel, Pervasiva e Embarcada
    Computação Quântica
    Engenharia de Desempenho
    Escalonamento e Balanceamento de Carga
    Internet das Coisas (IoT)
    Linguagens, Compiladores e Ferramentas para Alto Desempenho
    Memória Compartilhada Distribuída (DSM)
    Modelagem e Simulação de Arquiteturas e Sistemas Paralelos/Sistemas Distribuídos
    Redes e Protocolos de Comunicação de Alto Desempenho
    Simulação de Arquiteturas e Sistemas Paralelos
    Sistemas de Arquivos e Entrada e Saída de Alto Desempenho
    Sistemas de Banco de Dados Paralelos e Distribuídos
    Sistemas de Memória
    Sistemas Operacionais
    Sistemas Tolerantes a Falhas
    Software Básico para Computação Paralela e Distribuída
    Técnicas e Métodos de Extração de Paralelismo
    Teste e Depuração de Programas Concorrentes
    Virtualização

Submissões

A submissão de artigos para o SSCAD-WIC 2025 deve ser feita pelo
sistema JEMS da SBC. Os artigos submetidos devem ser escritos em
português ou inglês e obedecer ao limite de 8 páginas (incluindo
figuras, tabelas e referências) seguindo o formato da SBC para
submissão de artigos.  Coordenação do SSCAD-WIC

    Gabriel P. Silva (Universidade Federal do Rio de Janeiro) — gabriel@ic.ufrj.br
    Samuel Ferraz (Universidade Federal de Mato Grosso do Sul) — samuel.ferraz@ufms.br

Comitê de Programa (a confirmar)

    Adenauer Yamin (Universidade Católica de Pelotas/Universidade Federal de Pelotas)
    Alexandre Carissimi (Universidade Federal do Rio Grande do Sul)
    Anderson Faustino (Universidade Estadual de Maringá)
    André Du Bois (Universidade Federal de Pelotas)
    Andriele Busatto do Carmo (Universidade do Vale do Rio dos Sinos)
    Arthur Lorenzon (Universidade Federal do Rio Grande do Sul)
    Calebe Bianchini (Universidade Presbiteriana Mackenzie)
    Claudio Schepke (Universidade Federal do Pampa)
    Dalvan Griebler (Pontifícia Universidade Católica do Rio Grande do Sul)
    Diego Leonel Cadette Dutra (Universidade Federal do Rio de Janeiro)
    Edson Tavares de Camargo (Universidade Tecnológica Federal do Paraná)
    Edson Luiz Padoin (Universidade Regional do Noroeste do Estado do Rio Grande do Sul)
    Edward Moreno (Universidade Federal de Sergipe)
    Emilio Francesquini (Universidade Federal do ABC)
    Fabíola M. C. de Oliveira (Universidade Federal do ABC)
    Fabrício Góes (University of Leicester)
    Gabriel Nazar (Universidade Federal do Rio Grande do Sul)
    Gabriel P. Silva (Universidade Federal do Rio de Janeiro)
    Gerson Geraldo H. Cavalheiro (Universidade Federal de Pelotas)
    Guilherme Galante (Universidade Estadual do Oeste do Paraná)
    Guilherme Koslovski (Universidade do Estado de Santa Catarina)
    Hélio Guardia (Universidade Federal de São Carlos)
    Henrique Cota de Freitas (Pontifícia Universidade Católica de Minas Gerais)
    Hermes Senger (Universidade Federal de São Carlos)
    João Fabrício Filho (Universidade Tecnológica Federal do Paraná)
    Jorge Barbosa (Universidade do Vale do Rio dos Sinos)
    José Saito (Universidade Federal de São Carlos/Centro Universitário Campo Limpo Paulista)
    Josemar Souza (Universidade do Estado da Bahia)
    Joubert Lima (Universidade Federal de Ouro Preto)
    Juliano Foleiss (Universidade Tecnológica Federal do Paraná)
    Kalinka Castelo Branco (Instituto De Ciências Matemáticas e de Computação – USP)
    Leonardo Pinho (Universidade Federal do Pampa)
    Liana Duenha (Universidade Federal de Mato Grosso do Sul)
    Lucas Mello Schnorr (Universidade Federal do Rio Grande do Sul)
    Lucas Wanner (Universidade Estadual de Campinas)
    Luciano Senger (Universidade Estadual de Ponta Grossa)
    Luis Carlos De Bona (Universidade Federal do Paraná)
    Luiz Carlos Albini (Universidade Federal do Paraná)
    Marcelo Lobosco (Universidade Federal de Juiz de Fora)
    Marcio Oyamada (Universidade Estadual do Oeste do Paraná)
    Marco Wehrmeister (Universidade Tecnológica Federal do Paraná)
    Marco Antonio Zanata Alves (Universidade Federal do Paraná)
    Marcus Botacin (Texas A&M University)
    Maria Clicia Castro (Universidade Estadual do Rio de Janeiro)
    Mario Dantas (Universidade Federal de Juiz de Fora)
    Mateus Rutzig (Universidade Federal de Santa Maria)
    Matheus Souza (Pontifícia Universidade Católica de Minas Gerais)
    Márcio Castro (Universidade Federal de Santa Catarina)
    Márcio Kreutz (Universidade Federal do Rio Grande do Norte)
    Monica Pereira (Universidade Federal do Rio Grande do Norte)
    Nahri Moreano (Universidade Federal de Mato Grosso do Sul)
    Newton Will (Universidade Tecnológica Federal do Paraná)
    Odorico Mendizabal (Universidade Federal de Santa Catarina)
    Omar Cortes (Instituto Federal do Maranhão)
    Paulo Cesar Santos (Universidade Federal do Paraná)
    Rafaela Brum (Universidade Federal Fluminense)
    Renato Ishii (Universidade Federal de Mato Grosso do Sul)
    Ricardo da Rocha (Universidade Federal de Catalão)
    Ricardo Menotti (Universidade Federal de São Carlos)
    Rodolfo Azevedo (Universidade Estadual de Campinas)
    Rodrigo Campiolo (Universidade Tecnológica Federal do Paraná)
    Rodrigo Righi (Universidade do Vale do Rio dos Sinos)
    Rogério Gonçalves (Universidade Tecnológica Federal do Paraná)
    Samuel Ferraz (Universidade Federal do Mato Grosso do Sul)
    Sairo Santos (Universidade Federal Rural do Semi-Árido)
    Sarita Bruschi (Instituto de Ciências Matemáticas e de Computação – USP)
    Sergio Carvalho (Universidade Federal de Goiás)
    Tiago Ferreto (Pontifícia Universidade Católica Rio Grande do Sul)
    Tiago Heinrich (Universidade Federal do Paraná)
    Vinícius Vitor dos Santos Dias (Universidade Federal de Lavras)
    Vinícius Garcia (Universidade Federal do Paraná)
    Vinícius Garcia Pinto (Universidade Federal do Rio Grande)
    Wagner Zola (Universidade Federal do Paraná)
    Wanderson Roger Azevedo Dias (Instituto Federal de Rondônia)

Patrocinadores:
Diamante:

Parceiro:
Organização:
Promoção:
Financiamento:

    Chamada de Trabalhos – Trilha Principal Chamada de Trabalhos –
    Workshop sobre Educação em Arquitetura de Computadores (WEAC)
    Chamada de Trabalhos SSCAD-WIC Comitês Concurso de Teses e
    Dissertações em Arquitetura de Computadores e Computação de Alto
    Desempenho (SSCAD-CTD) Hospedagem Local Minicursos Principal

Copyright ©2025 XXVI SSCAD 2025 . All rights reserved. Powered by
WordPress & Designed by Bizberg Themes

* *The Paper*                                                       :ignore:
** Frontpage                                                        :ignore:

#+BEGIN_EXPORT latex
\makeatletter
\let\orgtitle\@title
\makeatother
\title{\orgtitle}

\author{
Francisco Pegoraro Etcheverria\inst{1},
Rayan Raddatz de Matos\inst{1},\\
Kenichi Brumati\inst{1},
Lucas Mello Schnorr\inst{1}
}

\address{Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS)\\
   Caixa Postal 15.064 -- 91.501-970 -- Porto Alegre -- RS -- Brazil}
#+END_EXPORT

#+LaTeX: \maketitle

** Abstract                                                         :ignore:

#+begin_abstract
This work investigates load balancing and scalability in
High-Performance Computing (HPC) systems using the Mandelbrot Set as a
benchmark. Due to its irregular and compute-intensive workload, the
Mandelbrot Set is ideal for evaluating dynamic workload distribution
strategies. We present a custom MPI-based implementation employing a
client-server model with dynamic load balancing, where idle workers
request new work from a coordinator. Our contributions include
implementing this parallel application and a performance analysis
highlighting load imbalance and system scalability.  The main results
indicate that the granularity factor significantly influences load
balance, and its choice depends on the selected fractal region.
#+end_abstract

** Introduction

# *[Context/Load Balancing]*
High-Performance Computing (HPC) systems are increasingly complex and
extensive cite:dongarra2024. They accommodate applications that
frequently have irregular workloads and are computationally
demanding. One of the significant concerns when dealing with irregular
workloads is achieving a balanced workload distribution among the
computational resources available to maximize performance and resource
usage. The load balancing task has a significant role in the
application performance when executing it in the cluster, ensuring
that each process in the system does approximately the same work
during the program execution.

# *[Mandelbrot]*
An everyday activity to study the correct usage of an HPC system is to
verify if the application achieves adequate load balancing amongst its
processes. This activity is particularly more challenging for
irregular workloads as the dynamic nature of the load makes standard
static partitioning techniques unsuitable. The Mandelbrot Set
cite:mandelbrot1980 is a typical application with an irregular
workload because it computes the points belonging to the set. The
application computes, for each point $c$ in the complex plane, the
application of the function $f(z) = z^2 + c$, starting from $z =
0$. The point $c$ belongs to the Mandelbrot Set if the computation of
such an equation is bounded and does not diverge to infinity. For each
pixel in a 2D image, an application needs to calculate the response
for that function to compute and generate this set.

# *[What is this work?/What we will do about the things we introduced?]*
The Mandelbrot Set is a well-known compute-bound application and is
considered embarrassingly parallel, as all point calculations are
independent. Therefore, being highly parallelizable and
computationally intensive, it often serves as a benchmark for HPC to
measure the performance of a computational system. In this article, we
implement an MPI version of this application from scratch with a
dynamic load balancer where workers, when idle, contact the
coordinator for a new set of points to work on. We then study the load
balancing and scalability of this HPC application. Our significant
contributions include: (1) a multi-threaded application that
implements the Mandelbrot Set using a client-server architecture where
the server is an MPI application with dynamic load-balancing, (2) a
performance analysis with metrics to provide an overview and load
imbalance perspectives. Our main results indicate that the granularity
factor significantly influences load balance, and its choice depends
on the selected fractal region and the severity of how unbalanced the
load is.

This paper has the following organization. Section [[sec.related]]
presents other load balancing performance analysis of the Mandelbrot
set. Section [[sec.materials]] discusses materials and methods of our
work, including a detailed description of the observed
system. Section [[sec.results]] presents the experimental
results. Finally, Section [[sec.conclusion]] concludes this text with
future work.

** Related Work
<<sec.related>>

# *[References about load balancing]*
Load balancing is a widely studied field. Chenzhong and Francis
cite:xu2007load provide a general overview of load balancing
techniques for parallel computers. Sandeep cite:sandeep2008 more
specifically analyzes different load balancing algorithms and
concludes that while dynamic distributed load balancing algorithms are
generally considered better, the static algorithms are more
stable and predictable.  Another work cite:mohammed2020two proposes a
two-level dynamic load balancing to scientific applications that
operate with MPI+OpenMP and uses the Mandelbrot application as one of
its test cases because of the application's irregularity.
#+latex: %
#+latex: % *[References about the mandelbrot implementation]*
The Mandelbrot Set as an HPC application has also been vastly studied
and used as a benchmark in various works for its computationally
intensive, parallelizable, and irregular nature. For example, all
works considering the Mandelbrot Set, Gomez cite:gomez2020mpi has
conducted a case study comparing MPI against OpenMP. Another work
cite:ozen2015exploring has specifically explored dynamic parallelism
in OpenMP.  Yang cite:yang2011performance also studies specific
parameters to control load balancing among application processes.
#+latex: %
The related work demonstrates how pertinent the Mandelbrot Set is as a
benchmarking application. Again, our work employs this application
with a strong focus on performance analysis, general metrics (mean
client time, speedup, and efficiency), and a metric specific to
quantifying load imbalance (imbalance percentage), as we detail next.

** Materials and Methods
<<sec.materials>>
*** The multi-threaded client--server MPI application for the Mandelbrot Set

The implemented multi-threaded system adopts a Client--Server
architecture designed to parallelize the computation of the Mandelbrot
Set while enabling efficient load balancing across multiple computing
nodes. Figure [[fig:system-architecture]] illustrates the overall
architecture, highlighting the main threads, communication queues, and
data flow between components. The Client is responsible for managing
user interactions and rendering the fractal image produced by the
Server. When the user requests a new region, the client issues a
/payload/ to the Server. The Server comprises a central MPI coordinator
which receives the /payload/ from the client, discretizes the
workload into smaller problems, and dynamically distributes these
smaller problems to a pool of MPI workers by demand. From the start of
the Server, the workers approach the coordinator looking for work. By
reception of a smaller problem, the workers carry out the numerical
computations for each of them (Mandelbrot Set) before sending back to
the Server the /responses/, which are then redirected to the Client.

#+CAPTION: Multi-threaded system overview with processes, threads, and queues.
#+NAME: fig:system-architecture
[[./figures/system_architecture.png]]


Each interaction between the Client and the Server consists of the
exchange of /payload/ and /response/ objects. A /payload/ is a data
structure that specifies the region of the Mandelbrot Set to be
computed, including the bounds in the complex plane, the corresponding
screen coordinates, the /depth/, which is the maximum number of iterations
to apply in the Mandelbrot algorithm, and the /granularity/, which
determines the size of the square blocks into which the Server
partitions the workload. For example, a granularity of 5 means that
the Server will split the fractal space into several 5\times5 square
blocks. Each payload also includes an increasing generation number to
identify it in the case the Client sends several /payloads/ one after
another.
#+latex: %
The Server replies to a single /payload/ with several /response/ objects,
each carrying its corresponding payload, as well as the calculated
depth count at each pixel position. In addition to the depth counts,
the responses also include some metadata, such as the id of the worker
that computed it. By delivering results block by block, the Server
enables the Client to view the partial fractal regions without waiting
for the entire computation to complete.

We designed the Client to be responsive and highly interactive. As
shown in Figure [[fig:system-architecture]], the Client contains four
concurrent threads. The ~Main~ thread manages both rendering of the
fractal image and collection of user mouse and keyboard input. When a
new region is selected, ~Main~ constructs the corresponding payload and
pushes it to a dedicated queue.  The ~SendPayload~ thread dequeues payloads
from this queue and transmits them to the Server over a TCP
connection. Meanwhile, ~RecvResponse~ listens for incoming responses, and
enqueues them into a response queue. Finally, the ~ProcessResponse~
thread retrieves these responses and integrates them into the
displayed image by applying a coloring function to the calculated
depth count for each pixel, updating the pixel buffer
incrementally as results arrive.

On the Server side, the ~RecvPayload~ thread listens for Client payloads,
forwarding them to the ~DiscretizePayloads~ thread, which divides the
requested region into several payloads sized according to the
specified granularity. These are then placed into a queue, with
outdated payloads being discarded to prevent workers from computing
regions that are no longer relevant. As workers become available, they
request a new payload from the coordinator. The ~SendToWorker~ thread
dynamically assigns them payloads from the queue. Each worker
independently computes a response, producing the depth counts for all
pixels in that subregion. Once the response becomes ready, it is sent to
the ~RecvFromWorker~ thread, which enqueues it to a response queue. These
responses are then collected by ~SendResponse~, which sends them back to the
Client.

*** Hardware & Software configuration

We run all experiments at the /Parque Computacional de Alto Desempenho/
(PCAD) at INF/UFRGS. The Client executes on a single /draco/ node, while
the server executes on one to six /cei/ nodes. The draco node on which the
Client executes has two Intel Xeon E5‑2640 v2 processors at 2.00 GHz.
Each cei node, used for the compute-bound part, has two Intel Xeon
Silver 4116 processors at 2.10 GHz, providing 24 physical cores each
for a total of 144 physical cores. In all experiments, we have
exclusive access to the machines without any type of
virtualization. We also use the /performance/ frequency governor of the
=acpi-cpufreq=.  The MPI implementation was OpenMPI version 4.1.4 and
the Linux Kernel 6.1.0 with SMP support as released by the Debian 12
distribution. The Client--Server Ethernet network is 1Gbps, while the
MPI application executes in a 10Gbps Ethernet switch.

*** Experimental Project

We designed a set of experiments with various input parameters to
evaluate the performance, scalability and load balancing of the
application on the target system. These parameters were chosen to test
different computational characteristics of the application, enabling
assessing how the system behaves under different workloads. The
experiments consisted of rendering fractal images with a resolution of
1920\times1080 pixels, typical for full high definition screens.  Each
execution is the combination of a value of the following factors:
Granularity, Number of Workers, and Fractal Cases. The *Granularity*
factor has the six levels: [5\times5, 10\times10, 20\times20, 40\times40, 60\times60, 120\times120],
respectively resulting in [82944, 20736, 5184, 1296, 576, 144] tasks
for workers. Smaller blocks improve load balancing but increase
communication overhead. Larger blocks may lead to severe load
imbalance. The *Number of Workers* factor has also six levels: [1, 2, 3,
4, 5, 6], derived from the limitations of our platform, where each
number of worker /nodes/ contribute to 24 physical cores.  This
corresponds to a total of 24 to 144 MPI ranks (step of 24 cores),
enabling the evaluation of how well the application scales with
additional resources. Finally, the *Fractal Cases* has three levels:
[easy, default, hard]. Figure [[fig:fractal-regions]] illustrates
representative images of the final result. The /easy/ (maximum depth
of 1024) depicts a region where most points escape in only a few
iterations, testing the communication overhead, rather than
computational speed. The /default/ (150000) depicts a typical unbalanced
Mandelbrot fractal region, containing both points that are
computationally intensive, as well as many points that escape quickly,
stressing requirements for active load balancing. Finally, the /hard/
(300000) depicts a deep region that is computationally intensive but
balanced, so we can assess computational throughput. We manually
selected the three max depth values of each case so the execution time
remains bounded to an acceptable value.

#+CAPTION: The three fractal cases, with the corresponding maximum depth values.
#+NAME: fig:fractal-regions
#+ATTR_LATEX: :placement [htbp]
\begin{figure}[htbp]
\centering
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{./figures/region_easy.png}
\caption*{easy (1024)}
\end{minipage}%\hfill
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{./figures/region_default.png}
\caption*{default (150000)}
\end{minipage}%\hfill
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{./figures/region_hard.png}
\caption*{hard (300000)}
\end{minipage}
\end{figure}

With these factors, we adopt a Full Factorial Design cite:jain1990art, enabling the
verification of all possible combinations of factors, resulting in 108
distinct configurations (6\times6\times3). Each configuration has been executed
ten times so we can assess the experimental variability, and the
execution order has been randomized to avoid potential bias.
#+LATEX: %
All experiments consider a simplified Client as we
executed everything in the cluster without a graphical interface. Our
textual Client instead receives parameters through the command
line. The ~ProcessResponse~ thread is therefore absent, and the ~Main~
thread enqueues the payload and dequeues responses from the ~RecvResponse~
thread.

**** Code                                                       :noexport:
#+begin_src R :results output :session *R* :exports none :noweb yes :colnames yes
options(crayon.enabled=FALSE)
library(DoE.base)
library(tidyverse)

fator_granularity = c(5, 10, 20, 40, 60, 120)
fator_nodes = 1:6
fator_coordinates = c("easy", "default", "hard")

fac.design(nfactors = 3,
           replications = 10,
           repeat.only = FALSE,
           randomize = TRUE,
           seed=0,
           nlevels=c(length(fator_granularity),
                     length(fator_nodes),
                     length(fator_coordinates)),
           factor.names=list(
             granularity = fator_granularity,
             nodes = fator_nodes,
             coordinates = fator_coordinates
           )) |>
  as_tibble() |>
  mutate(resolution = '1920x1080') |>
  mutate(depth = case_when(coordinates == "easy" ~ "X",
                           coordinates == "default" ~ "Y",
                           coordinates == "hard" ~ "Z")) |>
  mutate_at(vars(granularity:depth), as.character) |>
  select(granularity, nodes, coordinates, depth, resolution, Blocks) |>
  write_csv("projeto_experimental_francisco.csv", progress=FALSE)
#+end_src

#+RESULTS:
: creating full factorial with 108 runs ...

*** Observability

We manually instrument the code of the Client and Server to collect
and combine specific events and derive both execution time and load
balancing metrics. In the Client, we register the elapsed time between
the creation of each payload and the arrival of the first response, as
well as the last response. These metrics enable us to verify the
latency of the application as well as total perceived time from the
user perspective. In the server, we measured the time between a
payload being received and its discretization, the first and last
responses being received by the ~RecvFromWorker~ thread, and the moments these
responses are sent to the Client in the ~SendResponse~ thread. This
information allow us to verify the discretization cost, and the amount
of compute time from the perspective of the coordinator. Finally, in
each MPI worker, we measured the individual times to compute each
payload, their pixel and depth counts, as well as the aggregate of
these values. This information allows us to verify the load balance
among workers.

** Results
<<sec.results>>

We present the performance evaluation of our multi-threaded MPI
application based on the experiments described earlier. We focus on
four key metrics: the mean client time, speedup, efficiency, and
imbalance percentage. The *Mean Client Time* represents the total time
taken for the Client to receive the fully computed fractal for each
case (/payload/). The *Speedup* measures the ratio of the mean Client time with 
a single node for a given case and granularity setting to the mean Client 
time with another number of nodes for that same setting. That is, for a given number of 
nodes $n$, $S(n) = \frac{T(1)}{T(n)}$. We emphasize that our speedup
metric is relative to the number of nodes rather than processors. 
#+latex: %
Our server architecture is asymmetric, which necessitates a careful definition of 
ideal performance and efficiency. The baseline configuration on a single node uses 
23 workers and one coordinator, while each additional node contributes 24 workers. 
This results in a worker count for $n$ nodes of $24n - 1$.
Standard efficiency calculations using node count would yield misleading values above 
1.0 due to this uneven worker distribution.
Therefore, we normalize our metrics based on worker count rather than node count. 
We define the ideal speedup as $S_{ideal}(n) = \frac{24n - 1}{23}$, and *Efficiency*
as $E(n) = \frac{S(n)}{S_{ideal}(n)}$.
This method of computing $S$ and $E$ ensures that perfect linear scaling as workers are added results in efficiency = $1.0$, 
enabling fair comparison across configurations.
#+latex: %
Finally, the *Imbalance Percentage*
cite:derose2007detecting depicts how unevenly the computational
workload is distributed among workers. Lower values are better. It is
calculated as:
#+begin_export latex
\begin{equation}
\text{Imbalance Percentage} = \frac{L_{\text{max}} - L_{\text{avg}}}{L_{\text{max}}} \times \frac{n}{n-1}
\end{equation}
#+end_export
where $L_{\text{max}}$ is the computation time of the slowest worker,
$L_{\text{avg}}$ is the average computation time across all workers, and
$n$ is the number of workers. We picked the median value from the 10 trials.
#+latex: %
In our analysis we focus solely on Client times, which directly
reflect user-perceived performance, as the coordinator metrics closely
mirror client-side values. We also focus on worker-level timings,
which reveal the degree of load balancing achieved.

Figures [[fig:client-time]], [[fig:client-speedup]] and [[fig:client-efficiency]]
depict the time, speedup and efficiency results. We see that
performance appears to scale well with the addition of nodes for the
/default/ and /hard/ cases, provided an adequate granularity (nor low nor
high). The granularity 20 appears to be the best, with an efficiency
of around 0.98 in the /hard/ case, and approximately 0.85 in the /default/
case. This is likely due to it presenting a good trade‑off
between the payload size and the number of payloads, with small
communication overhead while providing good load balancing.

#+CAPTION: Mean Client time for each of the three cases (/payloads/).
#+NAME: fig:client-time
[[./figures/client_time.png]]

#+CAPTION: Speedup and ideal speedup for each case.
#+NAME: fig:client-speedup
[[./figures/client_speedup.png]]


This interpretation can be confirmed in Figure
[[fig:imbalance-percentage]], which shows generally better load balancing
for lower granularities. The load balancing at higher granularity values 
tends to degrade as the number of nodes increases. The
/default/ case in particular seems to suffer from more worker imbalance
than the /hard/ case, due to the fractal region having a mix of very
easy and very hard regions.

In contrast, the /easy/ case shows a different trend: higher
granularities consistently perform better, and increasing node counts
worsen performance. Because most points in this region escape in only
a few iterations, computation becomes inexpensive, and the bottleneck
is communication.  As such, lower granularities lead to higher
overhead, which seems to grow worse as more nodes are added. This
effect is especially visible at granularity 5 (see Figure
[[fig:client-time]] for instance): in the /default/ and /hard/ cases,
performance worsens past 3 nodes, nearly matching the times observed
in the /easy/ case. We conclude that the performance is limited by
communication rather than computation time at such low granularities.

#+CAPTION: Efficiency for each case.
#+NAME: fig:client-efficiency
[[./figures/client_efficiency.png]]




#+CAPTION: Median Imbalance Percentage for each case.
#+NAME: fig:imbalance-percentage
[[./figures/imbalance_percentage.png]]




Imbalance is also high across granularities in the /easy/ case, as the
work is so light that some workers can finish a payload and request
another, while other workers are still waiting for their next payload.

** Conclusion
<<sec.conclusion>>

This work presented a dynamic, multi-threaded MPI-based implementation
of the Mandelbrot Set to study load balancing and scalability in HPC
systems. Through extensive experimentation, we demonstrated that
workload granularity plays a crucial role in performance, with optimal
values depending on the computational characteristics of the fractal
region.
#+latex: %
These results show that scaling depends on the balance between
computation and communication costs. For harder fractal regions, the
system scales very well with additional nodes when granularity is
appropriately chosen, with granularity 20 striking the best
balance. However, for simpler regions, communication overhead
dominates and additional nodes can even reduce performance.
#+latex: %
These insights highlight the importance of tuning granularity based on
workload characteristics to achieve efficient parallel execution.  As
future work, we plan to investigate varying granularity values based
on neighborhood fractal depth and its impact on performance and load
balance.

** Acknowledgments
:PROPERTIES:
:UNNUMBERED: t
:END:

We thank FAPERGS and CNPq for their financial support, which included
scientific initiation scholarships from both FAPERGS (PROBIC) and CNPq
(PBIC).  We also thank UFRGS for all institutional support. We also
extend our thanks to the Parallel and Distributed Processing Research
Group (GPPD) for access to the PCAD cluster resources.

** References                                                        :ignore:

# See next section to understand how refs.bib file is created.
bibliographystyle:sbc.bst
[[bibliography:refs.bib]]

* Bib file is here                                                 :noexport:

Tangle this file with C-c C-v t

#+begin_src bibtex :tangle refs.bib
@book{jain1990art,
  title={The art of computer systems performance analysis},
  author={Jain, Raj},
  year={1990},
  publisher={john wiley \& sons}
}

  @article{yang2011performance,
    title={Performance-based parallel loop self-scheduling using hybrid OpenMP and MPI programming on multicore SMP clusters},
    author={Yang, Chao-Tung and Wu, Chao-Chin and Chang, Jen-Hsiang},
    journal={Concurrency and Computation: Practice and Experience},
    volume={23},
    number={8},
    pages={721--744},
    year={2011},
    publisher={Wiley Online Library}
  }

@inproceedings{ozen2015exploring,
author = {Ozen, Guray and Ayguade, Eduard and Labarta, Jesus},
title = {Exploring dynamic parallelism in OpenMP},
year = {2015},
isbn = {9781450340144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2832105.2832113},
doi = {10.1145/2832105.2832113},
abstract = {GPU devices are becoming a common element in current HPC platforms due to their high performance-per-Watt ratio. However, developing applications able to exploit their dazzling performance is not a trivial task, which becomes even harder when they have irregular data access patterns or control flows. Dynamic Parallelism (DP) has been introduced in the most recent GPU architecture as a mechanism to improve applicability of GPU computing in these situations, resource utilization and execution performance. DP allows to launch a kernel within a kernel without intervention of the CPU. Current experiences reveal that DP is offered to programmers at the expenses of an excessive overhead which, together with its architecture dependency, makes it difficult to see the benefits in real applications.In this paper, we propose how to extend the current OpenMP accelerator model to make the use of DP easy and effective. The proposal is based on nesting of teams constructs and conditional clauses, showing how it is possible for the compiler to generate code that is then efficiently executed under dynamic runtime scheduling. The proposal has been implemented on the MACC compiler supporting the OmpSs task--based programming model and evaluated using three kernels with data access and computation patterns commonly found in real applications: sparse matrix vector multiplication, breadth-first search and divide--and--conquer Mandelbrot. Performance results show speed-ups in the 40x range relative to versions not using DP.},
booktitle = {Proceedings of the Second Workshop on Accelerator Programming Using Directives},
articleno = {5},
numpages = {8},
keywords = {programming models, dynamic parallelism, compilers, OpenMP, OpenACC, OmpSs, GPGPU, CUDA},
location = {Austin, Texas},
series = {WACCPD '15}
}

  @article{dongarra2024,
    author    = {Dongarra, Jack and Keyes, David E.},
    title     = {The co-evolution of computational physics and high-performance computing},
    journal   = {Nature Reviews Physics},
    year      = {2024},
    url       = {https://www.nature.com/articles/s42254-024-00750-z}
  }


  @article{gomez2020mpi,
    title={MPI vs OpenMP: A case study on parallel generation of Mandelbrot set},
    author={G{\'o}mez, Ernesto Soto},
    journal={Innovation and Software},
    volume={1},
    number={2},
    pages={12--26},
    year={2020}
  }

@book{xu2007load,
author = {Xu, Chenzhong and Lau, Francis C. M.},
title = {Load Balancing in Parallel Computers: Theory and Practice},
year = {2013},
isbn = {1475770669},
publisher = {Springer Publishing Company, Incorporated},
edition = {1st},
abstract = {Load Balancing in Parallel Computers: Theory and Practice is about the essential software technique of load balancing in distributed memory message-passing parallel computers, also called multicomputers. Each processor has its own address space and has to communicate with other processors by message passing. In general, a direct, point-to-point interconnection network is used for the communications. Many commercial parallel computers are of this class, including the Intel Paragon, the Thinking Machine Cm-5, and the Ibm Sp2. Load Balancing in Parallel Computers: Theory and Practice presents a comprehensive treatment of the subject using rigorous mathematical analyses and practical implementations. The focus is on nearest-neighbor load balancing methods in which every processor at every step is restricted to balancing its workload with its direct neighbours only. Nearest-neighbor methods are iterative in nature because a global balanced state can be reached through processors' successive local operations. Since nearest-neighbor methods have a relatively relaxed requirement for the spread of local load information across the system, they are flexible in terms of allowing one to control the balancing quality, effective for preserving communication locality, and can be easily scaled in parallel computers with a direct communication network. Load Balancing in Parallel Computers: Theory and Practice serves as an excellent reference source and may be used as a text for advanced courses on the subject.}
}

  @inproceedings{mohammed2020two,
    title={Two-level dynamic load balancing for high performance scientific applications},
    author={Mohammed, Ali and Cavelan, Aur{\'e}lien and Ciorba, Florina M and Cabez{\'o}n, Rub{\'e}n M and Banicescu, Ioana},
    booktitle={SIAM Conference on Parallel Processing for Scientific Computing},
    year={2020},
  }

  @article{mandelbrot1980,
  author = {Mandelbrot, Benoit B.},
  title = { “Fractal Aspects of the Iteration of Z → z $\Lambda$(1-Z) for Complex $\Lambda$ and Z”},
  journal = {Annals of the New York Academy of Sciences},
  volume = {357},
  number = {1},
  pages = {249-259},
  year = {1980}
  }

  @article{sandeep2008,
    title     = {Performance Analysis of Load Balancing Algorithms},
    author    = {Sandeep Sharma and  Sarabjit Singh and  Meenakshi Sharma},
    country	= {},
    institution	= {},
    journal   = {International Journal of Civil and Environmental Engineering},
    volume    = {2},
    number    = {2},
    year      = {2008},
    pages     = {367},
    ee        = {https://publications.waset.org/pdf/5537},
    url   	= {https://publications.waset.org/vol/14},
    bibsource = {https://publications.waset.org/},
    issn  	= {eISSN: 1307-6892},
    publisher = {World Academy of Science, Engineering and Technology},
    index 	= {Open Science Index 14, 2008},
  }

@inproceedings{derose2007detecting,
  title={Detecting application load imbalance on high end massively parallel systems},
  author={DeRose, Luiz and Homer, Bill and Johnson, Dean},
  booktitle={European Conference on Parallel Processing},
  pages={150--159},
  year={2007},
  organization={Springer}
}

#+end_src
* Emacs setup                                                      :noexport:
# Local Variables:
# eval: (add-to-list 'load-path ".")
# eval: (require 'ox-extra)
# eval: (ox-extras-activate '(ignore-headlines))
# eval: (require 'org-ref)
# eval: (require 'doi-utils)
# eval: (add-to-list 'org-latex-packages-alist '("" "url") t)
# eval: (add-to-list 'org-latex-packages-alist '("" "sbc-template") t)
# eval: (add-to-list 'org-latex-packages-alist '("AUTO" "babel" t ("pdflatex")))
# eval: (setq org-latex-pdf-process (list "latexmk -pdf %f"))
# eval: (add-to-list 'org-export-before-processing-hook (lambda (be) (org-babel-tangle)))
# End:
