# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+TITLE: Load Balancing Mandelbrot Article
#+AUTHOR: Francisco Pegoraro Etcheverria, Rayan Raddatz de Matos, Kenichi Brumati, Lucas Mello Schnorr

#+STARTUP: overview indent
#+LANGUAGE: en
#+OPTIONS: H:3 creator:nil timestamp:nil skip:nil toc:nil num:t ^:nil ~:~
#+OPTIONS: author:nil title:nil date:nil
#+TAGS: noexport(n) deprecated(d) ignore(i)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [12pt]

#+LATEX_HEADER: \sloppy

# PDF generation can be done by make (thanks Luka Stanisic)
#   or C-c C-e l p (thanks Vinicius Garcia)

* Chamada de Trabalhos SSCAD-WIC                                   :noexport:

O Workshop de Iniciação Científica em Arquitetura de Computadores e
Computação de Alto Desempenho (SSCAD-WIC) é um evento anual, realizado
em conjunto com o Simpósio em Sistemas Computacionais de Alto
Desempenho (SSCAD) desde 2007, oferecendo uma oportunidade para os
alunos de graduação apresentarem e discutirem seus trabalhos nos
tópicos de interesse do SSCAD.

Os artigos aceitos no evento serão publicados em formato digital e
apresentados apenas na modalidade oral. Os artigos poderão ser
redigidos em português ou inglês. O processo de submissão de trabalhos
é eletrônico através do sistema JEMS onde serão aceitos somente
arquivos no formato PDF. Os anais serão publicados na SBC OpenLib
(SOL).

Os três melhores artigos aceitos no SSCAD-WIC receberão premiação.
Datas Importantes

    Submissão de trabalhos:31/07/2025
    Notificação de aceitação: 19/09/2024
    Envio da versão final: 25/09/2024

Tópicos de Interesse

A chamada de trabalhos está aberta (mas não limitada) aos seguintes
tópicos de interesse:

    Algoritmos Paralelos e Distribuídos
    Aplicações de Computação de Alto Desempenho
    Big Data (fundamentos; infraestrutura; administração e gerenciamento; descoberta e mineração; segurança e privacidade; aplicações)
    Aprendizado de Máquina em Alto Desempenho
    Arquiteturas de Computadores
    Arquiteturas Avançadas, Dedicadas e específicas
    Avaliação, Medição e Predição de Desempenho
    Computação em Aglomerados de Computadores
    Computação Heterogênea
    Computação de Alto Desempenho em Grade e na Nuvem
    Computação Móvel de Alto Desempenho
    Computação Móvel, Pervasiva e Embarcada
    Computação Quântica
    Engenharia de Desempenho
    Escalonamento e Balanceamento de Carga
    Internet das Coisas (IoT)
    Linguagens, Compiladores e Ferramentas para Alto Desempenho
    Memória Compartilhada Distribuída (DSM)
    Modelagem e Simulação de Arquiteturas e Sistemas Paralelos/Sistemas Distribuídos
    Redes e Protocolos de Comunicação de Alto Desempenho
    Simulação de Arquiteturas e Sistemas Paralelos
    Sistemas de Arquivos e Entrada e Saída de Alto Desempenho
    Sistemas de Banco de Dados Paralelos e Distribuídos
    Sistemas de Memória
    Sistemas Operacionais
    Sistemas Tolerantes a Falhas
    Software Básico para Computação Paralela e Distribuída
    Técnicas e Métodos de Extração de Paralelismo
    Teste e Depuração de Programas Concorrentes
    Virtualização

Submissões

A submissão de artigos para o SSCAD-WIC 2025 deve ser feita pelo
sistema JEMS da SBC. Os artigos submetidos devem ser escritos em
português ou inglês e obedecer ao limite de 8 páginas (incluindo
figuras, tabelas e referências) seguindo o formato da SBC para
submissão de artigos.  Coordenação do SSCAD-WIC

    Gabriel P. Silva (Universidade Federal do Rio de Janeiro) — gabriel@ic.ufrj.br
    Samuel Ferraz (Universidade Federal de Mato Grosso do Sul) — samuel.ferraz@ufms.br

Comitê de Programa (a confirmar)

    Adenauer Yamin (Universidade Católica de Pelotas/Universidade Federal de Pelotas)
    Alexandre Carissimi (Universidade Federal do Rio Grande do Sul)
    Anderson Faustino (Universidade Estadual de Maringá)
    André Du Bois (Universidade Federal de Pelotas)
    Andriele Busatto do Carmo (Universidade do Vale do Rio dos Sinos)
    Arthur Lorenzon (Universidade Federal do Rio Grande do Sul)
    Calebe Bianchini (Universidade Presbiteriana Mackenzie)
    Claudio Schepke (Universidade Federal do Pampa)
    Dalvan Griebler (Pontifícia Universidade Católica do Rio Grande do Sul)
    Diego Leonel Cadette Dutra (Universidade Federal do Rio de Janeiro)
    Edson Tavares de Camargo (Universidade Tecnológica Federal do Paraná)
    Edson Luiz Padoin (Universidade Regional do Noroeste do Estado do Rio Grande do Sul)
    Edward Moreno (Universidade Federal de Sergipe)
    Emilio Francesquini (Universidade Federal do ABC)
    Fabíola M. C. de Oliveira (Universidade Federal do ABC)
    Fabrício Góes (University of Leicester)
    Gabriel Nazar (Universidade Federal do Rio Grande do Sul)
    Gabriel P. Silva (Universidade Federal do Rio de Janeiro)
    Gerson Geraldo H. Cavalheiro (Universidade Federal de Pelotas)
    Guilherme Galante (Universidade Estadual do Oeste do Paraná)
    Guilherme Koslovski (Universidade do Estado de Santa Catarina)
    Hélio Guardia (Universidade Federal de São Carlos)
    Henrique Cota de Freitas (Pontifícia Universidade Católica de Minas Gerais)
    Hermes Senger (Universidade Federal de São Carlos)
    João Fabrício Filho (Universidade Tecnológica Federal do Paraná)
    Jorge Barbosa (Universidade do Vale do Rio dos Sinos)
    José Saito (Universidade Federal de São Carlos/Centro Universitário Campo Limpo Paulista)
    Josemar Souza (Universidade do Estado da Bahia)
    Joubert Lima (Universidade Federal de Ouro Preto)
    Juliano Foleiss (Universidade Tecnológica Federal do Paraná)
    Kalinka Castelo Branco (Instituto De Ciências Matemáticas e de Computação – USP)
    Leonardo Pinho (Universidade Federal do Pampa)
    Liana Duenha (Universidade Federal de Mato Grosso do Sul)
    Lucas Mello Schnorr (Universidade Federal do Rio Grande do Sul)
    Lucas Wanner (Universidade Estadual de Campinas)
    Luciano Senger (Universidade Estadual de Ponta Grossa)
    Luis Carlos De Bona (Universidade Federal do Paraná)
    Luiz Carlos Albini (Universidade Federal do Paraná)
    Marcelo Lobosco (Universidade Federal de Juiz de Fora)
    Marcio Oyamada (Universidade Estadual do Oeste do Paraná)
    Marco Wehrmeister (Universidade Tecnológica Federal do Paraná)
    Marco Antonio Zanata Alves (Universidade Federal do Paraná)
    Marcus Botacin (Texas A&M University)
    Maria Clicia Castro (Universidade Estadual do Rio de Janeiro)
    Mario Dantas (Universidade Federal de Juiz de Fora)
    Mateus Rutzig (Universidade Federal de Santa Maria)
    Matheus Souza (Pontifícia Universidade Católica de Minas Gerais)
    Márcio Castro (Universidade Federal de Santa Catarina)
    Márcio Kreutz (Universidade Federal do Rio Grande do Norte)
    Monica Pereira (Universidade Federal do Rio Grande do Norte)
    Nahri Moreano (Universidade Federal de Mato Grosso do Sul)
    Newton Will (Universidade Tecnológica Federal do Paraná)
    Odorico Mendizabal (Universidade Federal de Santa Catarina)
    Omar Cortes (Instituto Federal do Maranhão)
    Paulo Cesar Santos (Universidade Federal do Paraná)
    Rafaela Brum (Universidade Federal Fluminense)
    Renato Ishii (Universidade Federal de Mato Grosso do Sul)
    Ricardo da Rocha (Universidade Federal de Catalão)
    Ricardo Menotti (Universidade Federal de São Carlos)
    Rodolfo Azevedo (Universidade Estadual de Campinas)
    Rodrigo Campiolo (Universidade Tecnológica Federal do Paraná)
    Rodrigo Righi (Universidade do Vale do Rio dos Sinos)
    Rogério Gonçalves (Universidade Tecnológica Federal do Paraná)
    Samuel Ferraz (Universidade Federal do Mato Grosso do Sul)
    Sairo Santos (Universidade Federal Rural do Semi-Árido)
    Sarita Bruschi (Instituto de Ciências Matemáticas e de Computação – USP)
    Sergio Carvalho (Universidade Federal de Goiás)
    Tiago Ferreto (Pontifícia Universidade Católica Rio Grande do Sul)
    Tiago Heinrich (Universidade Federal do Paraná)
    Vinícius Vitor dos Santos Dias (Universidade Federal de Lavras)
    Vinícius Garcia (Universidade Federal do Paraná)
    Vinícius Garcia Pinto (Universidade Federal do Rio Grande)
    Wagner Zola (Universidade Federal do Paraná)
    Wanderson Roger Azevedo Dias (Instituto Federal de Rondônia)

Patrocinadores:
Diamante:

Parceiro:
Organização:
Promoção:
Financiamento:

    Chamada de Trabalhos – Trilha Principal Chamada de Trabalhos –
    Workshop sobre Educação em Arquitetura de Computadores (WEAC)
    Chamada de Trabalhos SSCAD-WIC Comitês Concurso de Teses e
    Dissertações em Arquitetura de Computadores e Computação de Alto
    Desempenho (SSCAD-CTD) Hospedagem Local Minicursos Principal

Copyright ©2025 XXVI SSCAD 2025 . All rights reserved. Powered by
WordPress & Designed by Bizberg Themes

* *The Paper*                                                       :ignore:
** Frontpage                                                        :ignore:

#+BEGIN_EXPORT latex
\makeatletter
\let\orgtitle\@title
\makeatother
\title{\orgtitle}

\author{
Francisco Pegoraro Etcheverria\inst{1},
Rayan Raddatz de Matos\inst{1},\\
Kenichi Brumati\inst{1},
Lucas Mello Schnorr\inst{1}
}

\address{Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS)\\
   Caixa Postal 15.064 -- 91.501-970 -- Porto Alegre -- RS -- Brazil
   \email{\{francisco.etcheverria, rayan.raddatz, kenichi.brumati, schnorr\}@inf.ufrgs.br}
   }
#+END_EXPORT

#+LaTeX: \maketitle

** Abstract                                                         :ignore:

#+begin_abstract
Put the abstract here.
#+end_abstract

** Introduction

# *[Context/Load Balancing]*
High Performance Computing (HPC) systems are increasingly complex and
large. They accommodate applications that frequently have irregular
workloads and are computationally demanding. When dealing with
irregular workloads, one of the major concerns is to achieve a
balanced workload distribution among the computational resources
available, to maximize both the performance and resource usage. The
load balancing task has an major role in the application performance
when executing it in the cluster, ensuring that each process in the
system does approximately the same work during the program execution.
The co-evolution of computational physics and high-performance
computing has led to the development of infrastructures, algorithms
and applications to address more complex computational challenges
cite:dongarra2024.

# *[Mandelbrot]*
A common activity to study the correct usage of a HPC system is to
verify if the application achieves adequate load balancing amongst its
processes. This activity is particularly harder for irregular
workloads as the dynamic nature of the load make common static
partitioning techniques unsuitable. The Mandelbrot Set
cite:mandelbrot1980 is a typical application that possesses an
irregular workload because of how it computes the points belonging to
the set. The application computes, for each point $c$ in the complex
plane, the application of the function $f(z) = z^2 + c$, starting from
$z = 0$. The point $c$ belongs to the Mandelbrot Set if the
computation of such equation is bounded and does not diverge to
infinity. An application needs to, for each pixel in a 2D image,
calculate the response for that function to compute and generate this
set.

# *[What is this work?/What we will do about the things we introduced?]*
The Mandelbrot Set is a very known compute-bound application and
considered as embarassangly parallel, as all points calculation are
independent. Therefore being highly parallelizable and computationally
intensive, it often serves as benchmark for HPC to measure the
performance a computational system.  In this article, we implement
from scratch an MPI version of this application with a dynamic load
balancer where workers, when idle, contact the coordinator for a new
set of points to work on. Our implementation is then used for our
major goal which is to to study the load balancing and scalability of
a HPC Systems from the perspective of this well-known benchmark. Our
major contributions include: (1) a multi-threaded application that
implements the Mandelbrot Set using a client-server architecture where
the server is a MPI application with dynamic load-balancing, (2) a
profound performance analysis using state-of-the-art performance
metrics for overview and load imbalance perspectives. Our main results
observed in a cluster with a W configuration is Z.

This paper has the following organization. Section [[sec.related]]
presents other load balancing performance analysis of the Mandelbrot
set. Section [[sec.materials]] discusses materials and methods of our
work, including the detailed description of the observed
system. Section [[sec.results]] presents the experimental
results. Finally, Section [[sec.conclusion]] concludes this text with
future work.

** Related Work
<<sec.related>>

# *[References about load balancing]*
Load balancing is a widely studied field, Chenzhong and Francis
cite:xu2007load provide a general overview of the techniques for
parallel computers. Sandeep cite:sandeep2008 more specifically
analyzes different load balancing algorithms and concludes that while
dynamic distributed load balancing algorithms are always generally
considered better, the static algorithms are more stable and
predictable.  Another work cite:mohammed2020two proposes a two-level
dynamic load balancing to scientific applications that operate with
MPI+OpenMP and uses the Mandelbrot application as one of its test
cases because of the application irregularity.
#+latex: %
#+latex: % *[References about the mandelbrot implementation]*
The Mandelbrot Set as an HPC application is also vastly studied and
used as a benchmark in a variety of works for its computationally
intensive, parallizable and irregular nature. For example, all works
considering the Mandelbrot Set, Gomez cite:gomez2020mpi has carried
out a case study to compare MPI against OpenMP.  Another work
cite:ozen2015exploring has specifically explored dynamic parallelism
in OpenMP. Yang cite:yang2011performance also study specific
parameters to control how load balancing actuates among application
processes.
#+latex: %
The related work demonstrates how pertinent is the usage of the
Mandelbrot Set as a benchmarking application. Our work employs once
again this application with a strong focus on the the performance
analysis, employing general metrics (mean client time, speedup, and
efficiency), and a metric specific to quantify the load imbalance
(imbalance percentage), as we detail next.

** Materials and Methods
<<sec.materials>>
*** The multi-threaded client-server MPI application for the Mandelbrot Set

The implemented multi-thread system adopts a Client--Server
architecture designed to parallelize the computation of the Mandelbrot
Set while enabling efficient load balancing across multiple computing
nodes. Figure [[fig:system-architecture]] illustrates the overall
architecture, highlighting the main threads, communication queues, and
data flow between components. The Client is responsible for managing
user interactions and rendering the fractal image produced by the
server. When the user requests a new region, the client issues a
/payload/ to the server. The Server comprises a central MPI coordinator
which receives the /payload/ from the client, discretizes the
workload into smaller problems, and dynamically distributes these
smaller problems to a pool of MPI workers by demand. From the start of
the server, the workers approach the coordinator looking for work. By
reception of a smaller problem, the workers carry out the numerical
computations for each of them (Mandelbrot Set) before sending back to
the Server the /responses/, which are then redirected to the Client.

#+CAPTION: Multi-thread system overview with processes, threads, and queues.
#+NAME: fig:system-architecture
[[./figures/system_architecture.png]]


Each interaction between the Client and the Server consists of the
exchange of /payload/ and /response/ objects. A /payload/ is a data
structure that specifies the region of the Mandelbrot Set to be
computed, including the bounds in the complex plane, the corresponding
screen coordinates, the /depth/, which is maximum number of iterations
to apply in the Mandelbrot algorithm, and the /granularity/, which
determines the size of the square blocks into which the server
partitions the workload. For example, a granularity of 5 means that
the server will split the fractal space into several 5\times5 square
blocks. Each payload also includes an increasing generation number to
identify it in the case the Client sends several /payloads/ in after the
another.

The Server replies to a single /payload/ with several /response/ objects,
each carrying its corresponding payload, as well as the calculated
depth count at each pixel position. In addition to the depth counts,
the responses also include some metadata, such as the id of the worker
that computed it. By delivering results block by block, the Server
enables the Client to view the partial fractal regions without waiting
for the entire computation to complete.

We designed the Client to be responsive and highly interactive. As
shown in Figure [[fig:system-architecture]], the Client contains four
concurrent threads. The ~Main~ thread manages both rendering of the
fractal image and collection of user mouse and keyboard input. When a
new region is selected, ~Main~ constructs the corresponding payload and
pushes it to a dedicated queue.  The ~Net Send~ thread dequeues payloads
from this queue and transmits them to the Server over a TCP
connection. Meanwhile, ~Net Recv~ listens for incoming responses, and
enqueues them into a response queue. Finally, the ~Process Response~
thread retrieves these responses and integrates them into the
displayed image by applying a coloring function to the calculated
depth count for each pixel, updating the pixel buffer
incrementally as results arrive.

On the Server side, the ~Net Recv~ thread listens for Client payloads,
forwarding them to the ~Discretize payloads~ thread, which divides the
requested region into several payloads sized according to the
specified granularity. These are then placed into a queue, with
outdated payloads being discarded to prevent workers from computing
regions that are no longer relevant. As workers become available, they
request a new payload from the coordinator. The ~MPI Send~ thread
dynamically assigns them payloads from the queue. Each worker
independently computes a response, producing the depth counts for all
pixels in that subregion. Once the response becomes ready, it is sent to
the ~MPI Recv~ thread, which enqueues it to a response queue. These
responses are then collected by ~Net Send~, which sends them back to the
Client.

*** Hardware & Software configuration

We run all experiments at the /Parque Computacional de Alto Desempenho/
(PCAD) at INF/UFRGS. The Client executes on a single /draco/ node, while
the server executes on one to six /cei/ nodes. The draco node on which the
Client executes has two Intel Xeon E5‑2640 v2 processors at 2.00 GHz.
Each cei node, used for the compute-bound part, has two Intel Xeon
Silver 4116 processors at 2.10 GHz, providing 24 physical cores each
for a total of 144 physical cores. In all experiments, we have
exclusive access to the machines without any type of
virtualization. We also take time to pin each MPI rank to a physical
CPU core and use the /performance/ frequency governor of the
=acpi-cpufreq=.  The MPI implementation was OpenMPI version 4.1.4 and
the Linux Kernel 6.1.0 with SMP support as released by the Debian 12
distribution. The Client--Server Ethernet network is 1Gbps, while the
MPI application executes in a 10Gbps Ethernet switch.

*** Experimental Project

We designed a set of experiments with various input parameters to
evaluate the performance, scalability and load balancing of the
application on the target system. These parameters were chosen to test
different computational characteristics of the application, enabling
assessing how the system behaves under different workloads. The
experiments consisted of rendering fractal images with a resolution of
1920\times1080 pixels, typical for full high definition screens.  Each
execution is the combination of a value of the following factors:
Granularity, Number of Workers, and Fractal Cases. The *Granularity*
factor has the six levels: [5\times5, 10\times10, 20\times20, 40\times40, 60\times60, 120\times120],
respectively resulting in [82944, 20736, 5184, 1296, 576, 144] tasks
for workers. Smaller blocks improve load balancing but increase
communication overhead. Larger blocks may lead to severe load
imbalance. The *Number of Workers* factor has also six levels: [1, 2, 3,
4, 5, 6], derived from the limitations of our platform, where each
number of worker /nodes/ contribute to 24 physical cores.  This
corresponds to a total of 24 to 144 MPI ranks (step of 24 cores),
enabling the evaluation of how well the application scales with
additional resources. Finally, the *Fractal Cases* has three levels:
[easy, default, hard]. Figure [[fig:fractal-regions]] illustrates
representative images of the final result. The /easy/ (maximum depth
of 1024) depicts a region where most points escape in only a few
iterations, testing the communication overhead, rather than
computational speed. The /default/ (150000) depicts a typical unbalanced
Mandelbrot fractal region, containing both points that are
computationally intensive, as well as many points that escape quickly,
stressing requirements for active load balancing. Finally, the /hard/
(300000) depicts a deep region that is computationally intensive but
balanced, so we can assess computational throughput. We manually
selected the three max depth values of each case so the execution time
remains bounded to an acceptable value.

#+CAPTION: Default, Easy, and Hard fractal regions, from left to right.
#+NAME: fig:fractal-regions
#+ATTR_LATEX: :placement [htbp]
\begin{figure}[htbp]
\centering
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{./figures/region_easy.png}
\caption*{easy (1024)}
\end{minipage}%\hfill
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{./figures/region_default.png}
\caption*{default (150000)}
\end{minipage}%\hfill
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{./figures/region_hard.png}
\caption*{hard (300000)}
\end{minipage}
\end{figure}

With these factors, we adopt a Full Factorial Design cite:jain1990art, enabling the
verification of all possible combinations of factors, resulting in 108
distinct configurations (6\times6\times3). Each configuration has been executed
ten times so we can assess the experimental variability, and the
execution order has been randomized to avoid potential bias.
#+LATEX: %
All experiments consider a simplified experimental Client as we
executed everything in the cluster without a graphical interface. Our
textual Client instead receives parameters through the command
line. As such, the ~Process Response~ thread is absent, and the ~Main~
thread enqueues the payload and dequeues responses from the ~Net Recv~
thread.

**** Code                                                       :noexport:
#+begin_src R :results output :session *R* :exports none :noweb yes :colnames yes
options(crayon.enabled=FALSE)
library(DoE.base)
library(tidyverse)

fator_granularity = c(5, 10, 20, 40, 60, 120)
fator_nodes = 1:6
fator_coordinates = c("easy", "default", "hard")

fac.design(nfactors = 3,
           replications = 10,
           repeat.only = FALSE,
           randomize = TRUE,
           seed=0,
           nlevels=c(length(fator_granularity),
                     length(fator_nodes),
                     length(fator_coordinates)),
           factor.names=list(
             granularity = fator_granularity,
             nodes = fator_nodes,
             coordinates = fator_coordinates
           )) |>
  as_tibble() |>
  mutate(resolution = '1920x1080') |>
  mutate(depth = case_when(coordinates == "easy" ~ "X",
                           coordinates == "default" ~ "Y",
                           coordinates == "hard" ~ "Z")) |>
  mutate_at(vars(granularity:depth), as.character) |>
  select(granularity, nodes, coordinates, depth, resolution, Blocks) |>
  write_csv("projeto_experimental_francisco.csv", progress=FALSE)
#+end_src

#+RESULTS:
: creating full factorial with 108 runs ...

*** Observability

We manually instrument the code of the Client and Server to collect
and combine specific events and derive both execution time and load
balancing metrics. In the Client, we register the elapsed time between
the creation of each payload and the arrival of the first response, as
well as the last response. These metrics enable us to verify the
latency of the application as well as total perceived time from the
user perspective. In the server, we measured the time between a
payload being received and its discretization, the first and last
responses being received by the ~MPI Recv~ thread, and the moments these
responses are sent to the Client in the ~Net Send~ thread. This
information allow us to verify the discretization cost, and the amount
of compute time from the perspective of the coordinator. Finally, in
each MPI worker, we measured the individual times to compute each
payload, their pixel and depth counts, as well as the aggregate sum of
these values. This information allow us to verify the load balance
among workers.

** Results
<<sec.results>>

We present the performance evaluation of our multi-thread MPI
applicaton based on the experiments described earlier. We focus on
four key metrics: the mean client time, speedup, efficiency, and
imbalance percentage. The *Mean Client Time* represents the total time
taken for the Client to receive the fully computed fractal for each
setting. The *Speedup*, a common metric in HPC, represents the ratio of
the mean Client time with a single node for a given region and
granularity setting (the baseline), to the mean Client time with
multiple nodes for that same setting.  We emphasize that our speedup
metric is relative to the number of nodes. The *Efficiency* represents
the speedup normalized by the number of nodes, indicating how well the
system scales with more nodes. Finally, the *Imbalance Percentage*
cite:derose2007detecting depicts how unevenly the computational
workload is distributed among workers. Lower values are better. It is
calculated as:
#+begin_export latex
\begin{equation}
\text{Imbalance Percentage} = \frac{L_{\text{max}} - L_{\text{avg}}}{L_{\text{max}}} \times \frac{n}{n-1}
\end{equation}
#+end_export
where $L_{\text{max}}$ is the computation time of the slowest worker,
$L_{\text{avg}}$ is the average computation time across all workers, and
$n$ is the number of workers.
#+latex: %
In our analysis we focus solely on Client times, which directly
reflect user-perceived performance, as the coordinator metrics closely
mirror client-side values. We also focus on worker-level timings,
which reveal the degree of load balancing achieved.



#+CAPTION: Mean Client time for each setting.
#+NAME: fig:client-time
[[./figures/client_time.png]]

#+CAPTION: Client speedup relative to 1 node for each setting.
#+NAME: fig:client-speedup
[[./figures/client_speedup.png]]

#+CAPTION: Client efficiency relative to 1 node for each setting.
#+NAME: fig:client-efficiency
[[./figures/client_efficiency.png]]

#+CAPTION: Imbalance percentage across trials on each setting.
#+NAME: fig:imbalance-percentage
[[./figures/imbalance_percentage.png]]

Examining Figures [[fig:client-time]], [[fig:client-speedup]] and [[fig:client-efficiency]],
performance appears to scale well with the addition of nodes for the /default/ and /hard/ cases, 
provided the granularity is not too low or too high. In particular, granularity 20 appears to 
perform the best in those cases, with an efficiency close to 1 in the /hard/ case, and 
approximately 0.88 in the /default/ case. This is likely due to there being a good trade‑off 
between the payload size and the number of payloads, keeping a low communication overhead while 
also balancing work between workers well. 

This is supported by Figure [[fig:imbalance-percentage]],
which shows generally better load balancing for lower granularities, with performance degrading 
at values over 40. This effect is higher the more nodes, and therefore workers, are present.
The /default/ case in particular seems to suffer from more worker imbalance
than the /hard/ case, due to the fractal region having a mix of very easy and very hard regions.

In contrast, the /easy/ case shows a different trend: higher granularities consistently perform 
better, and increasing node counts worsen performance. Because most points in this region escape 
in only a few iterations, computation becomes inexpensive, and the bottleneck is communication. 
As such, lower granularities lead to higher overhead, which seems to grow worse as more nodes are 
added. This effect is especially visible at granularity 5: in the /default/ and /hard/ cases, 
performance worsens past 3 nodes, nearly matching the times observed in the /easy/ case. This 
suggests that the performance is being capped by communication overhead rather than computation 
time at such low granularities. 

Imbalance is also high across granularities in the /easy/ case, as 
the work is so light that some workers can finish a payload and request another, while other 
workers are still waiting for their next payload.

These results show that scaling depends on the balance between computation
and communication costs. For harder fractal regions, the system scales very well 
with additional nodes when granularity is appropriately chosen, with granularity 20 striking 
the best balance. However, for simpler regions, communication overhead dominates 
and additional nodes can even reduce performance.

** Conclusion
<<sec.conclusion>>


** Acknowledgments
:PROPERTIES:
:UNNUMBERED: t
:END:

We would like to express our sincere gratitude to the Rio Grande do Sul Research Foundation (FAPERGS) and the Brazilian National Council for Scientific and
Technological Development (CNPq) for their financial support, which included scientific initiation scholarships from both FAPERGS (PROBIC) and CNPq (PBIC).
We thank the Federal University of Rio Grande do Sul (UFRGS) for all institutional support. We also extend our thanks to the Parallel and Distributed
Processing Research Group (GPPD) for access to the PCAD cluster resources, which were essential for carrying out this work.

** References                                                        :ignore:

# See next section to understand how refs.bib file is created.
bibliographystyle:sbc.bst
[[bibliography:refs.bib]]

* Bib file is here                                                 :noexport:

Tangle this file with C-c C-v t

#+begin_src bibtex :tangle refs.bib
@book{jain1990art,
  title={The art of computer systems performance analysis},
  author={Jain, Raj},
  year={1990},
  publisher={john wiley \& sons}
}

  @article{yang2011performance,
    title={Performance-based parallel loop self-scheduling using hybrid OpenMP and MPI programming on multicore SMP clusters},
    author={Yang, Chao-Tung and Wu, Chao-Chin and Chang, Jen-Hsiang},
    journal={Concurrency and Computation: Practice and Experience},
    volume={23},
    number={8},
    pages={721--744},
    year={2011},
    publisher={Wiley Online Library}
  }


  @inproceedings{ozen2015exploring,
    title={Exploring dynamic parallelism in openmp},
    author={Ozen, Guray and Ayguad{\'e}, Eduard and Labarta, Jes{\'u}s},
    booktitle={Proceedings of the Second Workshop on Accelerator Programming using Directives},
    pages={1--8},
    year={2015}
  }

  @article{dongarra2024,
    author    = {Dongarra, Jack and Keyes, David E.},
    title     = {The co-evolution of computational physics and high-performance computing},
    journal   = {Nature Reviews Physics},
    year      = {2024},
    url       = {https://www.nature.com/articles/s42254-024-00750-z}
  }


  @article{gomez2020mpi,
    title={MPI vs OpenMP: A case study on parallel generation of Mandelbrot set},
    author={G{\'o}mez, Ernesto Soto},
    journal={Innovation and Software},
    volume={1},
    number={2},
    pages={12--26},
    year={2020}
  }


  @book{xu2007load,
    title={Load balancing in parallel computers: theory and practice},
    author={Xu, Chenzhong and Lau, Francis CM},
    volume={381},
    year={2007},
    publisher={Springer}
  }

  @inproceedings{mohammed2020two,
    title={Two-level dynamic load balancing for high performance scientific applications},
    author={Mohammed, Ali and Cavelan, Aur{\'e}lien and Ciorba, Florina M and Cabez{\'o}n, Rub{\'e}n M and Banicescu, Ioana},
    booktitle={Proceedings of the 2020 SIAM Conference on Parallel Processing for Scientific Computing},
    pages={69--80},
    year={2020},
    organization={SIAM}
  }

  @article{mandelbrot1980,
  author = {Mandelbrot, Benoit B.},
  title = { “Fractal Aspects of the Iteration of Z → z $\Lambda$(1-Z) for Complex $\Lambda$ and Z”},
  journal = {Annals of the New York Academy of Sciences},
  volume = {357},
  number = {1},
  pages = {249-259},
  year = {1980}
  }



  @article{sandeep2008,
    title     = {Performance Analysis of Load Balancing Algorithms},
    author    = {Sandeep Sharma and  Sarabjit Singh and  Meenakshi Sharma},
    country	= {},
    institution	= {},
    journal   = {International Journal of Civil and Environmental Engineering},
    volume    = {2},
    number    = {2},
    year      = {2008},
    pages     = {367 - 370},
    ee        = {https://publications.waset.org/pdf/5537},
    url   	= {https://publications.waset.org/vol/14},
    bibsource = {https://publications.waset.org/},
    issn  	= {eISSN: 1307-6892},
    publisher = {World Academy of Science, Engineering and Technology},
    index 	= {Open Science Index 14, 2008},
  }

@inproceedings{derose2007detecting,
  title={Detecting application load imbalance on high end massively parallel systems},
  author={DeRose, Luiz and Homer, Bill and Johnson, Dean},
  booktitle={European Conference on Parallel Processing},
  pages={150--159},
  year={2007},
  organization={Springer}
}

#+end_src
* Emacs setup                                                      :noexport:
# Local Variables:
# eval: (add-to-list 'load-path ".")
# eval: (require 'ox-extra)
# eval: (ox-extras-activate '(ignore-headlines))
# eval: (require 'org-ref)
# eval: (require 'doi-utils)
# eval: (add-to-list 'org-latex-packages-alist '("" "url") t)
# eval: (add-to-list 'org-latex-packages-alist '("" "sbc-template") t)
# eval: (add-to-list 'org-latex-packages-alist '("AUTO" "babel" t ("pdflatex")))
# eval: (setq org-latex-pdf-process (list "latexmk -pdf %f"))
# eval: (add-to-list 'org-export-before-processing-hook (lambda (be) (org-babel-tangle)))
# End:
