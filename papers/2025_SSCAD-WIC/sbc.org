# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+TITLE: Load Balancing Mandelbrot Article
#+AUTHOR: Francisco Pegoraro Etcheverria, Rayan Raddatz de Matos, Kenichi Brumati, Lucas Mello Schnorr

#+STARTUP: overview indent
#+LANGUAGE: en
#+OPTIONS: H:3 creator:nil timestamp:nil skip:nil toc:nil num:t ^:nil ~:~
#+OPTIONS: author:nil title:nil date:nil
#+TAGS: noexport(n) deprecated(d) ignore(i)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [12pt]

#+LATEX_HEADER: \sloppy

# PDF generation can be done by make (thanks Luka Stanisic)
#   or C-c C-e l p (thanks Vinicius Garcia)

* Chamada de Trabalhos SSCAD-WIC                                   :noexport:

O Workshop de Iniciação Científica em Arquitetura de Computadores e
Computação de Alto Desempenho (SSCAD-WIC) é um evento anual, realizado
em conjunto com o Simpósio em Sistemas Computacionais de Alto
Desempenho (SSCAD) desde 2007, oferecendo uma oportunidade para os
alunos de graduação apresentarem e discutirem seus trabalhos nos
tópicos de interesse do SSCAD.

Os artigos aceitos no evento serão publicados em formato digital e
apresentados apenas na modalidade oral. Os artigos poderão ser
redigidos em português ou inglês. O processo de submissão de trabalhos
é eletrônico através do sistema JEMS onde serão aceitos somente
arquivos no formato PDF. Os anais serão publicados na SBC OpenLib
(SOL).

Os três melhores artigos aceitos no SSCAD-WIC receberão premiação.
Datas Importantes

    Submissão de trabalhos:31/07/2025
    Notificação de aceitação: 19/09/2024
    Envio da versão final: 25/09/2024

Tópicos de Interesse

A chamada de trabalhos está aberta (mas não limitada) aos seguintes
tópicos de interesse:

    Algoritmos Paralelos e Distribuídos
    Aplicações de Computação de Alto Desempenho
    Big Data (fundamentos; infraestrutura; administração e gerenciamento; descoberta e mineração; segurança e privacidade; aplicações)
    Aprendizado de Máquina em Alto Desempenho
    Arquiteturas de Computadores
    Arquiteturas Avançadas, Dedicadas e específicas
    Avaliação, Medição e Predição de Desempenho
    Computação em Aglomerados de Computadores
    Computação Heterogênea
    Computação de Alto Desempenho em Grade e na Nuvem
    Computação Móvel de Alto Desempenho
    Computação Móvel, Pervasiva e Embarcada
    Computação Quântica
    Engenharia de Desempenho
    Escalonamento e Balanceamento de Carga
    Internet das Coisas (IoT)
    Linguagens, Compiladores e Ferramentas para Alto Desempenho
    Memória Compartilhada Distribuída (DSM)
    Modelagem e Simulação de Arquiteturas e Sistemas Paralelos/Sistemas Distribuídos
    Redes e Protocolos de Comunicação de Alto Desempenho
    Simulação de Arquiteturas e Sistemas Paralelos
    Sistemas de Arquivos e Entrada e Saída de Alto Desempenho
    Sistemas de Banco de Dados Paralelos e Distribuídos
    Sistemas de Memória
    Sistemas Operacionais
    Sistemas Tolerantes a Falhas
    Software Básico para Computação Paralela e Distribuída
    Técnicas e Métodos de Extração de Paralelismo
    Teste e Depuração de Programas Concorrentes
    Virtualização

Submissões

A submissão de artigos para o SSCAD-WIC 2025 deve ser feita pelo
sistema JEMS da SBC. Os artigos submetidos devem ser escritos em
português ou inglês e obedecer ao limite de 8 páginas (incluindo
figuras, tabelas e referências) seguindo o formato da SBC para
submissão de artigos.  Coordenação do SSCAD-WIC

    Gabriel P. Silva (Universidade Federal do Rio de Janeiro) — gabriel@ic.ufrj.br
    Samuel Ferraz (Universidade Federal de Mato Grosso do Sul) — samuel.ferraz@ufms.br

Comitê de Programa (a confirmar)

    Adenauer Yamin (Universidade Católica de Pelotas/Universidade Federal de Pelotas)
    Alexandre Carissimi (Universidade Federal do Rio Grande do Sul)
    Anderson Faustino (Universidade Estadual de Maringá)
    André Du Bois (Universidade Federal de Pelotas)
    Andriele Busatto do Carmo (Universidade do Vale do Rio dos Sinos)
    Arthur Lorenzon (Universidade Federal do Rio Grande do Sul)
    Calebe Bianchini (Universidade Presbiteriana Mackenzie)
    Claudio Schepke (Universidade Federal do Pampa)
    Dalvan Griebler (Pontifícia Universidade Católica do Rio Grande do Sul)
    Diego Leonel Cadette Dutra (Universidade Federal do Rio de Janeiro)
    Edson Tavares de Camargo (Universidade Tecnológica Federal do Paraná)
    Edson Luiz Padoin (Universidade Regional do Noroeste do Estado do Rio Grande do Sul)
    Edward Moreno (Universidade Federal de Sergipe)
    Emilio Francesquini (Universidade Federal do ABC)
    Fabíola M. C. de Oliveira (Universidade Federal do ABC)
    Fabrício Góes (University of Leicester)
    Gabriel Nazar (Universidade Federal do Rio Grande do Sul)
    Gabriel P. Silva (Universidade Federal do Rio de Janeiro)
    Gerson Geraldo H. Cavalheiro (Universidade Federal de Pelotas)
    Guilherme Galante (Universidade Estadual do Oeste do Paraná)
    Guilherme Koslovski (Universidade do Estado de Santa Catarina)
    Hélio Guardia (Universidade Federal de São Carlos)
    Henrique Cota de Freitas (Pontifícia Universidade Católica de Minas Gerais)
    Hermes Senger (Universidade Federal de São Carlos)
    João Fabrício Filho (Universidade Tecnológica Federal do Paraná)
    Jorge Barbosa (Universidade do Vale do Rio dos Sinos)
    José Saito (Universidade Federal de São Carlos/Centro Universitário Campo Limpo Paulista)
    Josemar Souza (Universidade do Estado da Bahia)
    Joubert Lima (Universidade Federal de Ouro Preto)
    Juliano Foleiss (Universidade Tecnológica Federal do Paraná)
    Kalinka Castelo Branco (Instituto De Ciências Matemáticas e de Computação – USP)
    Leonardo Pinho (Universidade Federal do Pampa)
    Liana Duenha (Universidade Federal de Mato Grosso do Sul)
    Lucas Mello Schnorr (Universidade Federal do Rio Grande do Sul)
    Lucas Wanner (Universidade Estadual de Campinas)
    Luciano Senger (Universidade Estadual de Ponta Grossa)
    Luis Carlos De Bona (Universidade Federal do Paraná)
    Luiz Carlos Albini (Universidade Federal do Paraná)
    Marcelo Lobosco (Universidade Federal de Juiz de Fora)
    Marcio Oyamada (Universidade Estadual do Oeste do Paraná)
    Marco Wehrmeister (Universidade Tecnológica Federal do Paraná)
    Marco Antonio Zanata Alves (Universidade Federal do Paraná)
    Marcus Botacin (Texas A&M University)
    Maria Clicia Castro (Universidade Estadual do Rio de Janeiro)
    Mario Dantas (Universidade Federal de Juiz de Fora)
    Mateus Rutzig (Universidade Federal de Santa Maria)
    Matheus Souza (Pontifícia Universidade Católica de Minas Gerais)
    Márcio Castro (Universidade Federal de Santa Catarina)
    Márcio Kreutz (Universidade Federal do Rio Grande do Norte)
    Monica Pereira (Universidade Federal do Rio Grande do Norte)
    Nahri Moreano (Universidade Federal de Mato Grosso do Sul)
    Newton Will (Universidade Tecnológica Federal do Paraná)
    Odorico Mendizabal (Universidade Federal de Santa Catarina)
    Omar Cortes (Instituto Federal do Maranhão)
    Paulo Cesar Santos (Universidade Federal do Paraná)
    Rafaela Brum (Universidade Federal Fluminense)
    Renato Ishii (Universidade Federal de Mato Grosso do Sul)
    Ricardo da Rocha (Universidade Federal de Catalão)
    Ricardo Menotti (Universidade Federal de São Carlos)
    Rodolfo Azevedo (Universidade Estadual de Campinas)
    Rodrigo Campiolo (Universidade Tecnológica Federal do Paraná)
    Rodrigo Righi (Universidade do Vale do Rio dos Sinos)
    Rogério Gonçalves (Universidade Tecnológica Federal do Paraná)
    Samuel Ferraz (Universidade Federal do Mato Grosso do Sul)
    Sairo Santos (Universidade Federal Rural do Semi-Árido)
    Sarita Bruschi (Instituto de Ciências Matemáticas e de Computação – USP)
    Sergio Carvalho (Universidade Federal de Goiás)
    Tiago Ferreto (Pontifícia Universidade Católica Rio Grande do Sul)
    Tiago Heinrich (Universidade Federal do Paraná)
    Vinícius Vitor dos Santos Dias (Universidade Federal de Lavras)
    Vinícius Garcia (Universidade Federal do Paraná)
    Vinícius Garcia Pinto (Universidade Federal do Rio Grande)
    Wagner Zola (Universidade Federal do Paraná)
    Wanderson Roger Azevedo Dias (Instituto Federal de Rondônia)

Patrocinadores:
Diamante:

Parceiro:
Organização:
Promoção:
Financiamento:

    Chamada de Trabalhos – Trilha Principal Chamada de Trabalhos –
    Workshop sobre Educação em Arquitetura de Computadores (WEAC)
    Chamada de Trabalhos SSCAD-WIC Comitês Concurso de Teses e
    Dissertações em Arquitetura de Computadores e Computação de Alto
    Desempenho (SSCAD-CTD) Hospedagem Local Minicursos Principal

Copyright ©2025 XXVI SSCAD 2025 . All rights reserved. Powered by
WordPress & Designed by Bizberg Themes

* *The Paper*                                                       :ignore:
** Frontpage                                                        :ignore:

#+BEGIN_EXPORT latex
\makeatletter
\let\orgtitle\@title
\makeatother
\title{\orgtitle}

\author{
Francisco Pegoraro Etcheverria\inst{1},
Rayan Raddatz de Matos\inst{1},
Kenichi Brumati\inst{1},\\
Lucas Mello Schnorr\inst{1}
}

\address{
   Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS)\\
   Caixa Postal 15.064 -- 91.501-970 -- Porto Alegre -- RS -- Brazil
   \email{\{francisco.etcheverria, rayan.raddatz, kenichi.brumati, schnorr\}@inf.ufrgs.br}
   }
#+END_EXPORT

#+LaTeX: \maketitle

** Abstract                                                         :ignore:

#+begin_abstract
Put the abstract here.
#+end_abstract

** Introduction                                                      :RA:KE:

High Performance Computing (HPC) systems have becoming more and more
complex and parallels over the years to accommodate the
applications that are usually irregulars and computationally
demanding. When dealing with this type of irregular applications, one
of the major concerns is the workload distribution among the
computational resources available in order to maximize the performance
and stability. This task, called load balancing, performs
a important role in the performance of a system when executing a
parallel application, ensuring that each processor in the systems does
approximately the same work during the programs execution. *[Context/Load Balancing]*

_We used a MPI implementation of the Mandelbrot application in order to
study the load balancing and scalability of a HPC Systems *[Change later with our objective]*_. The Mandelbrot Set is a set of points $c$
in the complex plane for which the application of the function $f(z) =
z^2 + c$
starting from $z = 0$ is bounded and does not diverge to
infinity cite:mandelbrot1980. To compute and generate this set, an
application needs to, for each pixel in a 2D image, calculate the
value based on the equation previously shown. The application that
does calculation work is highly parallelizable and computationally
intensive, often serving as benchmark for HPC used to measure the
performance a computational system. *[Mandelbrot]*

*[What is this work?/What we will do about the things we introduced?]*

This paper is organized as follows. Section [[sec.related]] presents related
work. Section [[sec.materials]] discusses all
the materials and methods of our work, including the detailed
description of the observed system. Section [[sec.results]] presents the
experimental results. Finally, Section [[sec.conclusion]] concludes
this work.
** Related Work
Load balancing is a widely studied field, cite:sandeep2008 analyzes
different load balancing algorithms and concludes that while dynamic
distributed load balancing algorithms are always generally considered
better, the static algorithms are more stables and predictable.
cite:mohammed2020two proposes a two-level dynamic load balancing to
scientific applications that operate with MPI+OpenMP and uses the
Mandelbrot application as one of its test cases because of the application irregularity.
*[References about load balancing]*

The Mandelbrot application is also vastly studied and used as a
benchmark in a variety of works for its computationally intensive,
parallizable and irregular nature.
*[References about the mandelbrot implementation]*

<<sec.related>>

** Materials and Methods                                             :FR:LU:
<<sec.materials>>
*** Observed System
**** General overview

The implemented system adopts a client–server architecture designed to parallelize
the computation of the Mandelbrot set while enabling efficient load balancing across
multiple computing nodes. The client is responsible for managing user interactions and
rendering the fractal image produced by the server. When the user requests a new region,
the client issues a request to the server. A central coordinator on the server receives
this request, discretizes the workload into smaller tasks, and dynamically distributes
the resulting tasks to a pool of MPI workers, which perform the numerical computations
for the Mandelbrot set.

Figure [[fig:system-architecture]] illustrates the overall
architecture, highlighting the main threads, communication queues, and data flow between
components.

#+CAPTION: System overview with processes, threads, and queues.
#+NAME: fig:system-architecture
[[./figures/system_architecture.png]]

Each interaction between the client and the server consists of the exchange of /payload/ and 
/response/ objects. A /payload/ is a data structure that specifies the region of the Mandelbrot 
set to be computed, including the bounds in the complex plane, the corresponding screen 
coordinates, the maximum number of iterations to apply in the Mandelbrot algorithm, 
and the /granularity/, which determines the size of the square blocks into which the server 
partitions the workload. For example, a granularity of 5 means that the server will split the
fractal space into several 5x5 square blocks. Each payload also includes an increasing generation 
number to identify it.

The server replies to a /payload/ with several /response/ objects, each carrying its corresponding
payload, as well as the calculated iteration count at each pixel position. In
addition to the iteration counts, responses also include some metadata, such as the id of the worker 
that computed it. By delivering results block by block, the server enables the client to view the
partial fractal regions without waiting for the entire computation to complete.

The client was designed to be responsive and highly interactive. As shown in figure
[[fig:system-architecture]], the client contains four concurrent threads. The ~Main~ thread manages 
both rendering of the fractal image and collection of user mouse and keyboard input. When a new region is selected,
~Main~ constructs the corresponding payload and pushes it to a dedicated queue. 
The ~Net Send~ thread dequeues payloads from this queue and transmits them to the server over a 
TCP connection. Meanwhile, ~Net Recv~ listens for incoming responses, and enqueues them 
into a response queue. Finally, the ~Process Response~ thread retrieves these responses and 
integrates them into the displayed image by applying a coloring function to the calculated 
iteration count for each pixel, updating the pixel buffer incrementally as results arrive.

On the server side, the ~Net Recv~ thread listens for client payloads, forwarding them to the 
~Discretize payloads~ thread, which divides the requested region into several payloads, each covering 
a smaller square area according to the specified granularity. These are then placed into a queue, 
with outdated payloads being discarded to prevent workers from computing
regions that are no longer relevant to the client. As workers become available, they
request a new payload from the coordinator. The ~MPI Send~ thread dynamically assigns them new 
payloads from the queue. Each worker independently computes its assigned payload, 
producing iteration counts for all pixels in that subregion. Once the response is computed, it is 
sent to the ~MPI Recv~ thread, which enqueues it to a response queue. These responses 
are then collected by ~Net Send~, which sends them back to the client. By returning 
results incrementally, the server ensures that the client can begin updating the display without 
waiting for the entire region to be completed.

**** Computational aspects
- Workload discretization etc
- MPI implementation

*** Hardware & Software configuration

All experiments were conducted at the PCAD cluster at UFRGS. The client ran on a single /draco/
node, while the server ran on 1 to 6 /cei/ nodes. 

- Each cei node was equipped with two Intel Xeon Silver 4116 processors at 2.10 GHz, providing a
  total of 24 physical cores. Within this server setup, each MPI Rank was pinned to a CPU core. 
  The MPI implementation was OpenMPI version 4.1.4.

- The client ran on a draco node with two Intel Xeon E5‑2640 v2 processors at 2.00 GHz.

*** Experimental Project
**** Input parameters

To evaluate the performance, scalability and load balancing of the system, we designed a set of 
experiments with various input parameters. These parameters were chosen to test different 
computational characteristics of the application, allowing us to assess how the system behaves 
under different workloads.

The experiments consisted of rendering different fractal images at **1920x1080** pixels.
For each run, the following parameters varied:

- **Granularity:** Values of [5, 10, 20, 40, 60, 120] were tested, resulting in [82,944, 20,736,
  5,184, 1,296, 576, 144] tasks for workers, respectively. Smaller block sizes improve
  load balancing but increase communication overhead, while larger granularities may lead to load 
  imbalance.

- **Number of Server Nodes:** We evaluated setups using 1 to 6 nodes, each contributing 24 physical
  cores. This corresponds to a total of 24 to 144 MPI ranks, allowing us to evaluate how well the 
  server scales as resources are added.

- **Fractal regions:** Three sets of coordinates and max depth values were chosen.
  - **Default:** A typical unbalanced Mandelbrot fractal region, containing both points that are 
    computationally intensive, as well as many points that escape quickly, stressing load 
    balancing.
  - **Easy:** A region where most points escape in only a few iterations, testing the communication
    overhead, rather than computational speed.
  - **Hard:** A deep region that is computationally intensive but balanced, testing computational
    throughput.

#+CAPTION: Default, Easy, and Hard fractal regions side by side
#+NAME: fig:fractal-regions
#+ATTR_LATEX: :placement [htbp]
\begin{figure}[htbp]
\centering
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{./figures/region_default.png}
\caption*{Default - Max depth of 150,000}
\end{minipage}%
\hfill
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{./figures/region_easy.png}
\caption*{Easy - Max depth of 1024}
\end{minipage}%
\hfill
\begin{minipage}{0.33\textwidth}
\centering
\includegraphics[width=\textwidth]{./figures/region_hard.png}
\caption*{Hard - Max depth of 300,000}
\end{minipage}
\end{figure}

**** Observability

To measure performance and load balancing, the client and server were modified to collect and 
log times and other metrics.

On the client, we recorded the elapsed time between the creation of each payload 
and the arrival of the first response, as well as the last response.

On the server, we measured the time between a payload being received and its discretization, as 
well as the first and last responses being received by the ~MPI Recv~ thread, and the times these
were sent to the client in the ~Net Send~ thread.

On each worker, we measured the individual times to compute each payload, their pixel counts and
iteration counts, as well as the aggregate sum of these values.

**** Design of Experiments

Experiments were conducted using a modified experimental client, which did not perform graphical 
rendering, and did not capture user mouse/keyboard input, instead receiving parameters through 
the command line. As such, the ~Process Response~ thread was removed, and the ~Main~ thread simply 
enqueued the payload and dequeued responses from the ~Net Recv~ thread.

The design followed a full factorial design across the parameters. All possible combinations of 
factors were evaluated, resulting in 108 distinct configurations. Each configuration was 
executed 10 times to reduce the impact of variability, and the order of runs was randomized to 
avoid potential bias.

***** Code                                                     :noexport:
#+begin_src R :results output :session *R* :exports none :noweb yes :colnames yes
options(crayon.enabled=FALSE)
library(DoE.base)
library(tidyverse)

fator_granularity = c(5, 10, 20, 40, 60, 120)
fator_nodes = 1:6
fator_coordinates = c("easy", "default", "hard")

fac.design(nfactors = 3,
           replications = 10,
           repeat.only = FALSE,
           randomize = TRUE,
           seed=0,
           nlevels=c(length(fator_granularity),
                     length(fator_nodes),
                     length(fator_coordinates)),
           factor.names=list(
             granularity = fator_granularity,
             nodes = fator_nodes,
             coordinates = fator_coordinates
           )) |>
  as_tibble() |>
  mutate(resolution = '1920x1080') |>
  mutate(depth = case_when(coordinates == "easy" ~ "X",
                           coordinates == "default" ~ "Y",
                           coordinates == "hard" ~ "Z")) |>
  mutate_at(vars(granularity:depth), as.character) |>
  select(granularity, nodes, coordinates, depth, resolution, Blocks) |>
  write_csv("projeto_experimental_francisco.csv", progress=FALSE)
#+end_src

#+RESULTS:
: creating full factorial with 108 runs ...

*** Evaluation procedure
** Results
<<sec.results>>

This section presents the performance evaluation of our fractal rendering system based on the 
experiments described earlier. We focus on four key metrics:

- Mean client time: The total time taken for the client to receive the fully computed fractal
  for each setting, averaged across the 10 trials. 

- Speedup: The ratio of the mean client time with a single node for a given region and granularity
  setting (the baseline), to the mean client time with multiple nodes for that same setting. 
  Note that the speedup is calculated relative to the number of nodes, not cores.

- Efficiency: The speedup normalized by the number of nodes, indicating how well the system 
  scales with more nodes.

- Imbalance Percentage: A measure of how unevenly the computational workload is distributed
  among workers. Lower values are better. It is calculated as:
  \begin{equation}
  \text{Imbalance Percentage} = \frac{L_{\text{max}} - L_{\text{avg}}}{L_{\text{max}}} \times \frac{n}{n-1}
  \end{equation}
  where $L_{\text{max}}$ is the computation time of the slowest worker, $L_{\text{avg}}$ is the average 
  computation time across all workers, and $n$ is the number of workers.

Although coordinator metrics were collected, they closely mirrored the client-side metrics.
We therefore focus on client times, which directly reflect user-perceived performance,
and worker-level timings, which reveal the degree of load balancing achieved.

#+CAPTION: Mean client time for each setting.
#+NAME: fig:client-time
[[./figures/client_time.png]]

#+CAPTION: Client speedup relative to 1 node for each setting.
#+NAME: fig:client-speedup
[[./figures/client_speedup.png]]

#+CAPTION: Client efficiency relative to 1 node for each setting.
#+NAME: fig:client-efficiency
[[./figures/client_efficiency.png]]

#+CAPTION: Imbalance percentage across trials on each setting.
#+NAME: fig:imbalance-percentage
[[./figures/imbalance_percentage.png]]

Examining Figures [[fig:client-time]], [[fig:client-speedup]] and [[fig:client-efficiency]],
performance appears to scale well with the addition of nodes for the /default/ and /hard/ cases, 
provided the granularity is not too low or too high. In particular, granularity 20 appears to 
perform the best in those cases, with an efficiency close to 1 in the /hard/ case, and 
approximately 0.88 in the /default/ case. This is likely due to there being a good trade‑off 
between the payload size and the number of payloads, keeping a low communication overhead while 
also balancing work between workers well. 

This is supported by Figure [[fig:imbalance-percentage]],
which shows consistently better load balancing for lower granularities, with performance degrading 
at values over 40. This effect is higher the more nodes, and therefore workers, are present.
The /default/ case in particular seems to suffer from more worker imbalance
than the /hard/ case, due to the fractal region having a mix of very easy and very hard regions.

In contrast, the /easy/ case shows a different trend: higher granularities consistently perform 
better, and increasing node counts worsen performance. Because most points in this region escape 
in only a few iterations, computation becomes inexpensive, and the bottleneck is communication. 
As such, lower granularities lead to higher overhead, which seems to grow worse as more nodes are 
added. This effect is especially visible at granularity 5: in the /default/ and /hard/ cases, 
performance worsens past 3 nodes, nearly matching the times observed in the /easy/ case. This 
suggests that the performance is being capped by communication overhead rather than computation 
time at such low granularities. 

Imbalance is also high across granularities in the /easy/ case, as 
the work is so light that some workers can finish a payload and request another, while other 
workers are still waiting for their next payload.

These results show that scaling depends on the balance between computation
and communication costs. For harder fractal regions, the system scales very well 
with additional nodes when granularity is appropriately chosen, with granularity 20 striking 
the best balance. However, for simpler regions, communication overhead dominates 
and additional nodes can even reduce performance.

** Conclusion
<<sec.conclusion>>



** Acknowledgments
:PROPERTIES:
:UNNUMBERED: t
:END:

We would like to express our sincere gratitude to the Rio Grande do Sul Research Foundation (FAPERGS) and the Brazilian National Council for Scientific and
Technological Development (CNPq) for their financial support, which included scientific initiation scholarships from both FAPERGS (PROBIC) and CNPq (PBIC).
We thank the Federal University of Rio Grande do Sul (UFRGS) for all institutional support. We also extend our thanks to the Parallel and Distributed
Processing Research Group (GPPD) for access to the PCAD cluster resources, which were essential for carrying out this work.

** References                                                        :ignore:

# See next section to understand how refs.bib file is created.
bibliographystyle:sbc.bst
[[bibliography:refs.bib]]

* Bib file is here                                                 :noexport:

Tangle this file with C-c C-v t
#+begin_src bibtex :tangle refs.bib
@inproceedings{mohammed2020two,
  title={Two-level dynamic load balancing for high performance scientific applications},
  author={Mohammed, Ali and Cavelan, Aur{\'e}lien and Ciorba, Florina M and Cabez{\'o}n, Rub{\'e}n M and Banicescu, Ioana},
  booktitle={Proceedings of the 2020 SIAM Conference on Parallel Processing for Scientific Computing},
  pages={69--80},
  year={2020},
  organization={SIAM}
}

@article{mandelbrot1980,
author = {Mandelbrot, Benoit B.},
title = { “Fractal Aspects of the Iteration of Z → z $\Lambda$(1-Z) for Complex $\Lambda$ and Z”},
journal = {Annals of the New York Academy of Sciences},
volume = {357},
number = {1},
pages = {249-259},
year = {1980}
}



@article{sandeep2008,
  title     = {Performance Analysis of Load Balancing Algorithms},
  author    = {Sandeep Sharma and  Sarabjit Singh and  Meenakshi Sharma},
  country	= {},
  institution	= {},
  journal   = {International Journal of Civil and Environmental Engineering},
  volume    = {2},
  number    = {2},
  year      = {2008},
  pages     = {367 - 370},
  ee        = {https://publications.waset.org/pdf/5537},
  url   	= {https://publications.waset.org/vol/14},
  bibsource = {https://publications.waset.org/},
  issn  	= {eISSN: 1307-6892},
  publisher = {World Academy of Science, Engineering and Technology},
  index 	= {Open Science Index 14, 2008},
}


#+end_src
* Emacs setup                                                      :noexport:
# Local Variables:
# eval: (add-to-list 'load-path ".")
# eval: (require 'ox-extra)
# eval: (ox-extras-activate '(ignore-headlines))
# eval: (require 'org-ref)
# eval: (require 'doi-utils)
# eval: (add-to-list 'org-latex-packages-alist '("" "url") t)
# eval: (add-to-list 'org-latex-packages-alist '("" "sbc-template") t)
# eval: (add-to-list 'org-latex-packages-alist '("AUTO" "babel" t ("pdflatex")))
# eval: (setq org-latex-pdf-process (list "latexmk -pdf %f"))
# eval: (add-to-list 'org-export-before-processing-hook (lambda (be) (org-babel-tangle)))
# End:
